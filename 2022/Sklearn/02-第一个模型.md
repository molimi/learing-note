## 鸢尾花(iris)数据集分析和分类

后面对Sklearn的学习主要以《Python机器学习基础教程》和《机器学习实战基于scikit-learn和tensorflow》，两本互为补充进行学习，下面是开篇的学习内容。

## 1 初识数据
iris数据集的中文名是安德森鸢尾花卉数据集，英文全称是Anderson’s Iris data set。iris包含150个样本，对应数据集的每行数据。每行数据包含每个样本的四个特征和样本的类别信息，所以iris数据集是一个150行5列的二维表。

通俗地说，iris数据集是用来给花做分类的数据集，每个样本包含了花萼长度、花萼宽度、花瓣长度、花瓣宽度四个特征（前4列），我们需要建立一个分类器，分类器可以通过样本的四个特征来判断样本属于山鸢尾、变色鸢尾还是维吉尼亚鸢尾（这三个名词都是花的品种）。

iris的每个样本都包含了品种信息，即目标属性（第5列，也叫target或label）。如下所示：
<img src ="https://img-blog.csdnimg.cn/889960571aaa46699d9ba26b21a5f118.png#pic_center" width = 48%>


1. 导入数据集
(1) 在线导入
调用`load_iris`函数来加载数据：
```python
from sklearn.datasets import load_iris

iris_dataset = load_iris()
```
load_iris 返回的iris 对象是一个Bunch 对象，与字典非常相似，里面包含键和值：
```python
print(iris_dataset.keys())
# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
```
```python
print(iris_dataset["DESCR"][:193])
```
DESCR 键对应的值是数据集的简要说明。这里给出说明的开头部分


target_names 键对应的值是一个字符串数组，里面包含我们要预测的花的品种：
```python
print("Target names: {}".format(iris_dataset["target_names"]))
# Target names: ['setosa' 'versicolor' 'virginica']
```
feature_names 键对应的值是一个字符串列表，对每一个特征进行了说明：
```python
print("Feature names: {}".format(iris_dataset['feature_names']))
# Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
```

数据包含在target 和data 字段中。data 里面是花萼长度、花萼宽度、花瓣长度、花瓣宽
度的测量数据，格式为NumPy 数组：
```python
print("Type of data: {}".format(type(iris_dataset['data'])))
# Type of data: <class 'numpy.ndarray'>
```
data 数组的每一行对应一朵花，列代表每朵花的四个测量数据：
```python
print("shape of data: {}".format(iris_dataset['data'].shape))
# shape of data: (150, 4)
```
可以看出，数组中包含150 朵不同的花的测量数据。前机器学习中的个体叫作样
本（sample），其属性叫作特征（feature）。data 数组的形状（shape）是样本数乘以特征
数。这是scikit-learn 中的约定，你的数据形状应始终遵循这个约定。下面给出前5 个样
本的特征数值：
```python
print("First five rows of data:\n{}".format(iris_dataset['data'][:5]))
# First five rows of data:
#   [[5.1 3.5 1.4 0.2]
#    [4.9 3.  1.4 0.2]
#    [4.7 3.2 1.3 0.2]
#    [4.6 3.1 1.5 0.2]
#    [5.  3.6 1.4 0.2]]
```
从数据中可以看出，前5 朵花的花瓣宽度都是0.2cm，第一朵花的花萼最长，是5.1cm。

target 数组包含的是测量过的每朵花的品种，也是一个NumPy 数组：

```python
print("Type of target:{}".format(type(iris_dataset['target'])))
# Type of target:<class 'numpy.ndarray'>
```
target 是一维数组，每朵花对应其中一个数据：

```python
print("Shape of target:{}".format(iris_dataset['target'].shape))
# Shape of target:(150,)
```
品种被转换成从 0 到 2 的整数：
```python
print("target:\n{}".format(iris_dataset['target']))
```

(2) 本地导入
本地导入就需要重新编码

```python
# 导入相关包
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets
```

```python
# 导入数据集
data = "../seaborn-data-master/iris.csv"

iris_local = pd.read_csv(data, usecols=[0, 1, 2, 3, 4])
iris_local = iris_local.dropna()    # 丢弃含空值的行、列
iris_local.head()
```
<img src ="https://img-blog.csdnimg.cn/585bd2637fc141fc97cd5491131edb52.png#pic_center" width = 48%>


```python
# 查看数据集信息
iris_local.info()
'''
<class 'pandas.core.frame.DataFrame'>
Int64Index: 150 entries, 0 to 149
Data columns (total 5 columns):
sepal_length    150 non-null float64
sepal_width     150 non-null float64
petal_length    150 non-null float64
petal_width     150 non-null float64
species         150 non-null object
dtypes: float64(4), object(1)
memory usage: 7.0+ KB
'''
```

```python
# 载入特征和标签集
X = iris_local[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]        # 等价于iris_dataset.data
y = iris_local['species']     # 等价于iris_dataset.target
```

```python
# 对标签集进行编码
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y = encoder.fit_transform(y)
print(y)
```

## 2 训练数据与测试数据
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
```

```python
print("X_train shape: {}".format(X_train.shape))        # X_train shape: (112, 4)
print("y_train shape: {}".format(y_train.shape))        # y_train shape: (112,)
```

```python
print("X_test shape: {}".format(X_test.shape))          # X_test shape: (38, 4)
print("y_test shape: {}".format(y_test.shape))          # y_test shape: (38,)
```

## 3 探索数据

```python
iris_local.describe()
```
<img src ="https://img-blog.csdnimg.cn/ed7a95248bd348a08bbfc972af9b3a28.png#pic_center" width = 48%>

箱线图描述了数据的分布情况，包括：上下界，上下四分位数和中位数，可以简单的查看数据的分布情况。
比如：上下四分位数相隔较远的话，一般可以很容易分为2类。
```python
iris_local.plot(kind='box', subplots=True, layout=(2, 2), sharex=False, sharey=False)
```
<img src ="https://img-blog.csdnimg.cn/6fa937adea1f49f0b4c0086d7cc26fa5.png#pic_center" width = 48%>

直方图，反馈的是数据的频度，一般常见的是高斯分布（正态分布）。
```python
iris_local.hist()
```
<img src ="https://img-blog.csdnimg.cn/39bea3ae41b84ed3b0e5452f4e1269ce.png#pic_center" width = 48%>

plot直接展示数据的分布情况，kde核密度估计对比直方图来看
```python
iris_local.plot()
```
<img src ="https://img-blog.csdnimg.cn/57ffb5123534484c8e2881b244694bb8.png#pic_center" width = 48%>

```python
iris_local.plot(kind='kde')
```

<img src ="https://img-blog.csdnimg.cn/210da5b6659843b184dc2f2d7f3d59cb.png#pic_center" width = 48%>


径向可视化是多维数据降维的可视化方法，不管是数据分析还是机器学习，降维是最基础的方法之一，通过降维，可以有效的减少复杂度。

径向坐标可视化是基于弹簧张力最小化算法。

它把数据集的特征映射成二维目标空间单位圆中的一个点，点的位置由系在点上的特征决定。把实例投入圆的中心，特征会朝圆中此实例位置（实例对应的归一化数值）“拉”实例。
```python
ax = pd.plotting.radviz(iris_local, 'species', colormap='brg')
ax.add_artist(plt.Circle((0,0), 1, color='r', fill = False))
```
<img src ="https://img-blog.csdnimg.cn/9debcdd538274cebbfb1a70a9d9c8a84.png#pic_center" width = 48%>


```python
#  通过Violinplot 和 Pointplot，分别从数据分布和斜率，观察各特征与品种之间的关系
# 设置颜色主题
antV = ['#1890FF', '#2FC25B', '#FACC14', '#223273', '#8543E0', '#13C2C2', '#3436c7', '#F04864'] 

# 绘制  Violinplot
f, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True)
sns.despine(left=True)

sns.violinplot(x='species', y='sepal_length', data=iris_local, palette=antV, ax=axes[(0, 0)])
sns.violinplot(x='species', y='sepal_width', data=iris_local, palette=antV, ax=axes[(0, 1)])
sns.violinplot(x='species', y='petal_length', data=iris_local, palette=antV, ax=axes[(1, 0)])
sns.violinplot(x='species', y='petal_width', data=iris_local, palette=antV, ax=axes[(1, 1)])

plt.show()
```

<img src ="https://img-blog.csdnimg.cn/20d61b1019c04e6180c596132f1ea27e.png#pic_center" width = 48%>

```python
# 绘制  pointplot
f, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True)
sns.despine(left=True)

sns.pointplot(x='species', y='sepal_length', data=iris_local, color=antV[0], ax=axes[0, 0])
sns.pointplot(x='species', y='sepal_width', data=iris_local, color=antV[0], ax=axes[0, 1])
sns.pointplot(x='species', y='petal_length', data=iris_local, color=antV[0], ax=axes[1, 0])
sns.pointplot(x='species', y='petal_width', data=iris_local, color=antV[0], ax=axes[1, 1])

plt.show()
```

<img src ="https://img-blog.csdnimg.cn/7b92d9a916c74ba3ad23949a10685ad0.png#pic_center" width = 48%>


```python
# 生成各特征之间关系的矩阵图
g = sns.pairplot(data=iris_local, palette=antV, hue= 'species')
```
<img src ="https://img-blog.csdnimg.cn/e26c092c05a34e068862d6720e515a6c.png#pic_center" width = 48%>


Andrews曲线将每个样本的属性值转化为傅里叶序列的系数来创建曲线，这对于检测时间序列数据中的异常值很有用。通过将每一类曲线标成不同颜色可以可视化聚类数据，属于相同类别的样本的曲线通常更加接近并构成了更大的结构。

```python
# 使用 Andrews Curves 将每个多变量观测值转换为曲线并表示傅立叶级数的系数，这对于检测时间序列数据中的异常值很有用。
plt.subplots(figsize = (10,8))
pd.plotting.andrews_curves(iris_local, 'species', colormap='cool')

plt.show()
```
<img src ="https://img-blog.csdnimg.cn/62139f3d01914b8cb18469f861b4d632.png#pic_center" width = 48%>


平行坐标可以看到数据中的类别以及从视觉上估计其他的统计量。# 使用平行坐标时，每个点用线段联接，每个垂直的线代表一个属性，# 一组联接的线段表示一个数据点。可能是一类的数据点会更加接近。

```python
pd.plotting.parallel_coordinates(iris_local, 'species', colormap = 'brg')
```

<img src ="https://img-blog.csdnimg.cn/4c06a1698aab4138bfcb55013b0c09a0.png#pic_center" width = 48%>

```python
#下面分别基于花萼和花瓣做线性回归的可视化：

g = sns.lmplot(data=iris_local, x='sepal_width', y='sepal_length', palette=antV, hue='species')
g = sns.lmplot(data=iris_local, x='petal_width', y='petal_length', palette=antV, hue='species')
```

<img src ="https://img-blog.csdnimg.cn/1aab58cc798e41ef8dcce5d344bbddb6.png#pic_center" width = 48%>

```python
#最后，通过热图找出数据集中不同特征之间的相关性，高正值或负值表明特征具有高度相关性：
fig=plt.gcf()
fig.set_size_inches(12, 8)
fig=sns.heatmap(iris_local.corr(), annot=True, cmap='GnBu', linewidths=1, linecolor='k', \
                square=True, mask=False, vmin=-1, vmax=1, cbar_kws={"orientation": "vertical"}, cbar=True)
```
<img src ="https://img-blog.csdnimg.cn/eb6c1156ee19484ab3e381cfc5791272.png#pic_center" width = 48%>


温馨提示：如果你想更方便快捷地了解数据的全貌，泣血推荐一个python库：`pandas_profiling`，这个库只需要一行代码就可以生成数据EDA报告。`pandas_profiling`基于pandas的DataFrame数据类型，可以简单快速地进行探索性数据分析

```python
import pandas_profiling as pp 
report = pp.ProfileReport(data)
report.to_file('report.html')
```
____

## 4 构建模型
scikit-learn 中有许多可用的分类算法。这里我们用的是 k 近邻分类器，这是一个很容易理解的算法。

要对一个新的数据点做出预测，算法会在训练集中寻找与这个新数据点距离最近的数据点，然后将找到的数据点的标签赋值给这个新数据点。

k 近邻算法中 k 的含义是，我们可以考虑训练集中与新数据点最近的任意k 个邻居（比如说，距离最近的3 个或5 个邻居），而不是只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。

k 近邻分类算法是在neighbors 模块KNeighborsClassifier 类中实现的。KNeighborsClassifier 最重要的参数就是邻居的数目，这里我们设为1：

```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)
```
想要基于训练集来构建模型，需要调用knn 对象的fit 方法，输入参数为X_train 和y_train，二者都是NumPy 数组，前者包含训练数据，后者包含相应的训练标签：
```python
knn.fit(X_train, y_train)
```

## 5 做出预测
想象一下，我们在野外发现了一朵鸢尾花，花萼长5cm 宽2.9cm，花瓣长1cm 宽0.2cm。这朵鸢尾花属于哪个品种？我们可以将这些数据放在一个NumPy 数组中，再次计
算形状，数组形状为样本数（1）乘以特征数（4）：
```python
X_new = np.array([[5, 2.9, 1, 0.2]])
print("X_new.shape: {}".format(X_new.shape))
```

注意，我们将这朵花的测量数据转换为二维NumPy 数组的一行，这是因为scikit-learn的输入数据必须是二维数组。我们调用knn 对象的predict 方法来进行预测：
```python
prediction = knn.predict(X_new)
print("Prediction: {}".format(prediction))          # Prediction: [0]
print("Predicted target name: {}".format(iris_dataset['target_names'][prediction]))     # Predicted target name: ['setosa']
```

根据我们模型的预测，这朵新的鸢尾花属于类别0，也就是说它属于setosa 品种。

## 6 评估模型

我们可以对测试数据中的每朵鸢尾花进行预测，并将预测结果与标签（已知的品种）进行对比。我们可以通过计算精度（accuracy）来衡量模型的优劣，精度就是品种预
测正确的花所占的比例：
```python
y_pred = knn.predict(X_test)
print("Test set predictions: \n {}".format(y_pred))
'''
Test set predictions: 
 [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0
 2]
 '''
```

```python
print("Test set score: {:.2f}".format(np.mean(y_pred == y_test)))   # Test set score: 0.97
```
这一步为了方便，也可以直接使用下面的方法来计算测试集的精度：
```python
print('Test set score: {:.2f}'.format(metrics.accuracy_score(y_pred, y_test)))   # Test set score: 0.97
```
我们还可以使用knn 对象的 score 方法来计算测试集的精度：
```python
print("Test set score: {:.2f}".format(knn.score(X_test, y_test)))   # Test set score: 0.97
```

对于这个模型来说，测试集的精度约为0.97，也就是说，对于测试集中的鸢尾花，我们的预测有 97% 是正确的。下面对比其他模型在该数据集的分类效果。


## 7 模型对比

**1. 使用全部特征**
```python
from sklearn.linear_model import LogisticRegression 
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics 
```

```python
# Support Vector Machine
model = svm.SVC()
model.fit(X_train, y_train)
prediction = model.predict(X_test)
print('The accuracy of the SVM is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test)))       # The accuracy of the SVM is: 0.974
```

```python
# Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)
prediction = model.predict(X_test)
print('The accuracy of the Logistic Regression is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test)))      # The accuracy of the Logistic Regression is: 0.974
```

```python
# Decision Tree
model=DecisionTreeClassifier()
model.fit(X_train, y_train)
prediction = model.predict(X_test)
print('The accuracy of the Decision Tree is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test)))     # The accuracy of the Logistic Regression is: 0.974
```

```python
# K-Nearest Neighbours
model=KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
prediction = model.predict(X_test)
print('The accuracy of the KNN is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test)))   # The accuracy of the Logistic Regression is: 0.974
```

**2. 使用部分特征**

上面使用了数据集的所有特征，下面将分别使用花瓣和花萼的尺寸：
```python
petal = iris_local[['petal_length', 'petal_width', 'species']]
train_p, test_p = train_test_split(petal, test_size=0.3, random_state=0) 
X_train_p = train_p[['petal_length', 'petal_width']]
y_train_p = train_p.species
X_test_p = test_p[['petal_length', 'petal_width']]
y_test_p = test_p.species

sepal = iris_local[['sepal_length', 'sepal_width', 'species']]
train_s, test_s = train_test_split(sepal, test_size=0.3, random_state=0)
X_train_s = train_s[['sepal_length','sepal_width']]
y_train_s = train_s.species
X_test_s = test_s[['sepal_length','sepal_width']]
y_test_s = test_s.species
```


```python
model=svm.SVC()

model.fit(X_train_p, y_train_p) 
prediction=model.predict(X_test_p) 
print('The accuracy of the SVM using Petals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_p)))   # The accuracy of the SVM using Petals is: 0.978

model.fit(X_train_s, y_train_s) 
prediction=model.predict(X_test_s) 
print('The accuracy of the SVM using Sepal is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_s)))    # The accuracy of the SVM using Sepal is: 0.800
```


```python
model = LogisticRegression()

model.fit(X_train_p, y_train_p) 
prediction = model.predict(X_test_p) 
print('The accuracy of the Logistic Regression using Petals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_p)))   # The accuracy of the Logistic Regression using Petals is: 0.978

model.fit(X_train_s, y_train_s) 
prediction = model.predict(X_test_s) 
print('The accuracy of the Logistic Regression using Sepals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_s)))   # The accuracy of the Logistic Regression using Sepals is: 0.822
```

```python
model=DecisionTreeClassifier()

model.fit(X_train_p, y_train_p) 
prediction = model.predict(X_test_p) 
print('The accuracy of the Decision Tree using Petals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_p)))     # The accuracy of the Decision Tree using Petals is: 0.956

model.fit(X_train_s, y_train_s) 
prediction = model.predict(X_test_s) 
print('The accuracy of the Decision Tree using Sepals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_s)))     # The accuracy of the Decision Tree using Sepals is: 0.644
```

```python
model=KNeighborsClassifier(n_neighbors=3) 

model.fit(X_train_p, y_train_p) 
prediction = model.predict(X_test_p) 
print('The accuracy of the KNN using Petals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_p)))   # The accuracy of the KNN using Petals is: 0.978

model.fit(X_train_s, y_train_s) 
prediction = model.predict(X_test_s) 
print('The accuracy of the KNN using Sepals is: {:.3f}'.format(metrics.accuracy_score(prediction, y_test_s)))   # The accuracy of the KNN using Sepals is: 0.733
```