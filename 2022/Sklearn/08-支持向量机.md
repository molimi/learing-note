## 1 介绍

SVM是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机（感知机有无数个，间隔最大只有一个）。SVM 适合中小型数据样本、非线性、高维的分类问题。
<font color=#9900CC><strong>SVM分为多分类和二分类</font></strong>

超平面：将不同样本（二分类）分隔开的平面，可图中实线。
最大超平面：除了将两类样本分别分割在该超平面的两侧，而且保证两侧距离超平面最近的样本点到超平面的距离被最大化了。
支持向量：样本中距离超平面最近的一些点，参考图中红色实心和红色空心点。
求解图中两条虚线的间隔最大化，因为距离最大犯错的几率比较小，更具鲁棒性。

在线演示：[http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)






## 2 sklearn实现
这里我们主要以核支持向量机为例，进行分类实验。首先导入库，并生成模拟数据集：
```python
import mglearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

X, y = make_blobs(centers=4, random_state=8)
y = y % 2

mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
```

<img src ="https://img-blog.csdnimg.cn/aeb2deb768794921a73971031d7b4e56.png#pic_center" width = 48%>

用于分类的线性模型只能用一条直线来划分数据点，对这个数据集无法给出较好的结果：
```python
from sklearn.svm import LinearSVC

linear_svm = LinearSVC().fit(X, y)

mglearn.plots.plot_2d_separator(linear_svm, X)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
```

<img src ="https://img-blog.csdnimg.cn/5bca957b861e4903870fba30269e028f.png#pic_center" width = 48%>

现在我们对输入特征进行扩展，比如说添加第二个特征的平方（feature1**2）作为一个
新特征。现在我们将每个数据点表示为三维点(feature0, feature1, feature1**2)，而
不是二维点(feature0, feature1)。这个新的表示可以画成下图中的三维散点图：


```python
from mpl_toolkits.mplot3d import Axes3D, axes3d

X_new = np.hstack([X, X[:, 1:] ** 2])       # 重组数据矩阵

figure = plt.figure()

# 3D可视化
ax = Axes3D(figure, elev=-152, azim=-26)
# 首先画出所有y==0的点，然后画出所有y==1的点
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b', cmap=mglearn.cm2, s=60)
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^', cmap=mglearn.cm2, s=60)
ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature1 ** 2")
```

<img src ="https://img-blog.csdnimg.cn/99469f75431046abb76ae1dd653ba7f2.png#pic_center" width = 48%>

在数据的新表示中，现在可以用线性模型（三维空间中的平面）将这两个类别分开。我们可以用线性模型拟合扩展后的数据来验证这一点：

```python
linear_svm_3d = LinearSVC().fit(X_new, y)
coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_

# 显示线性决策边界
figure = plt.figure()
ax = Axes3D(figure, elev=-152, azim=-26)
xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0] + 2, 50)
yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1] + 2, 50)

XX, YY = np.meshgrid(xx, yy)
ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]
ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b', cmap=mglearn.cm2, s=60)
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^', cmap=mglearn.cm2, s=60)
ax.set_xlabel("Feature 0")
ax.set_ylabel("Feature 1")
ax.set_zlabel("Feature1 ** 2")
```

<img src ="https://img-blog.csdnimg.cn/ded12b038bb24146bf70f62c154dd2e4.png#pic_center" width = 48%>

如果将线性SVM模型看作原始特征的函数，那么它实际上已经不是线性的了。它不是一条直线，而是一个椭圆，你可以在下图中看出：

```python
ZZ = YY ** 2
dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])
plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()], cmap=mglearn.cm2, alpha=0.5)

mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
```

<img src ="https://img-blog.csdnimg.cn/a70d13e96f074086be81cccc8e621689.png#pic_center" width = 48%>

向数据表示中添加非线性特征，可以让线性模型变得更强大。但是，通常来说我们并不知道要添加哪些特征，而且添加许多特征（比如100维特征空间所有可能的交互项）的计算开销可能会很大。幸运的是，有一种巧妙的数学技巧，让我们可以在更高维空间中学习分类器，而不用实际计算可能非常大的新的数据表示。这种技巧叫作核技巧（kernel trick），它的原理是直接计算扩展特征表示中数据点之间的距离（更准确地说是内积），而不用实际对扩展进行计算。

对于支持向量机，将数据映射到更高维空间中有两种常用的方法：一种是多项式核，在一定阶数内计算原始特征所有可能的多项式（比如feature1 ** 2 * feature2 ** 5）；另一种是径向基函数（radial basis function，RBF）核，也叫高斯核。高斯核有点难以解释，因为它对应无限维的特征空间。一种对高斯核的解释是它考虑所有阶数的所有可能的多项式，但阶数越高，特征的重要性越小。

下列代码将在forge数据集上训练SVM并创建此图：

```python
from sklearn.svm import SVC 

X, y = mglearn.tools.make_handcrafted_dataset()
svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)
mglearn.plots.plot_2d_separator(svm, X, eps=.5)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
# 画出支持向量
sv = svm.support_vectors_
# 支持向量的类别标签由dual_coef_的正负号给出
sv_labels = svm.dual_coef_.ravel() > 0
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
```

<img src ="https://img-blog.csdnimg.cn/3aa137b2a5594a37aa6e15cd68596223.png#pic_center" width = 48%>

在这个例子中，SVM给出了非常平滑且非线性（不是直线）的边界。这里我们调节了两个参数：C参数和gamma参数，下面我们将详细讨论。

gamma参数用于控制高斯核的宽度。它决定了点与点之间“靠近”是指多大的距离。C参数是正则化参数，与线性模型中用到的类似。它限制每个点的重要性。下面看改变这些参数会发生什么：

```python
fig, axes = plt.subplots(3, 3, figsize=(15, 10))

for ax, C in zip(axes, [-1, 0, 3]):
    for a, gamma in zip(ax, range(-1, 2)):
        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)

axes[0, 0].legend(["Class 0", "Class 1", "sv class 0", "sv class 1"], ncol=4, loc=(.9, 1.2))
```

<img src ="https://img-blog.csdnimg.cn/ecea07de835b47d4a5dfcc099b1bb9e2.png#pic_center" width = 48%>

从左到右，我们将参数gamma的值从0.1增加到10。gamma较小，说明高斯核的半径较大，许多点都被看作比较靠近。这一点可以在图中看出：左侧的图决策边界非常平滑，越向右的图决策边界更关注单个点。小的gamma值表示决策边界变化很慢，生成的是复杂度较低的模型，而大的gamma值则会生成更为复杂的模型。

从上到下，我们将参数C 的值从0.1增加到1000。与线性模型相同，C值很小，说明模型非常受限，每个数据点的影响范围都有限。你可以看到，左上角的图中，决策边界看起来几乎是线性的，误分类的点对边界几乎没有任何影响。再看左下角的图，增大C之后这些点对模型的影响变大，使得决策边界发生弯曲来将这些点正确分类。

我们将RBF 核SVM应用到乳腺癌数据集上。默认情况下，C=1，gamma=1/n_features：

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

svc = SVC()
svc.fit(X_train, y_train)

print("Accuracy on training set: {:.3f}".format(svc.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(svc.score(X_test, y_test)))
```

> Accuracy on training set: 1.00
> Accuracy on test set: 0.63

这个模型在训练集上的分数十分完美，但在测试集上的精度只有63%，存在相当严重的过拟合。虽然SVM 的表现通常都很好，但它对参数的设定和数据的缩放非常敏感。特别地，它要求所有特征有相似的变化范围。我们来看一下每个特征的最小值和最大值，它们绘制在对数坐标上：

```python
plt.plot(X_train.min(axis=0), 'o', label="min")
plt.plot(X_train.max(axis=0), '^', label="max")
plt.legend(loc=4)
plt.xlabel("Feature index")
plt.xlabel("Feature magnitude")
plt.yscale("log")
```

<img src ="https://img-blog.csdnimg.cn/67a7c73c827b4049a91c5b1d9c7d1ed9.png#pic_center" width = 48%>

从这张图中，我们可以确定乳腺癌数据集的特征具有完全不同的数量级。这对其他模型来
说（比如线性模型）可能是小问题，但对核SVM 却有极大影响。我们来研究处理这个问
题的几种方法。

解决这个问题的一种方法就是对每个特征进行缩放，使其大致都位于同一范围。核SVM常用的缩放方法就是将所有特征缩放到0 和1 之间。我们将在后面学习如何使用MinMaxScaler预处理方法来做到这一点，到时会给出更多细节。现在我们来“人工”做到这一点：
```python
# 计算训练集中每个特征的最小值
min_on_training = X_train.min(axis=0)
# 计算训练集每个特征的范围（最大值-最小值）
range_on_training = (X_train - min_on_training).max(axis=0)

# 减去最小值，然后除以范围
# 这样每个特征都收min=0, max=1
X_train_scaled = (X_train - min_on_training) / range_on_training
print("Minimum for each feature\n{}".format(X_train_scaled.min(axis=0)))
print("Maximum for each feature\n{}".format(X_train_scaled.max(axis=0)))
```
> Minimum for each feature
> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
>  0. 0. 0. 0. 0. 0.]
> Maximum for each feature
> [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
>  1. 1. 1. 1. 1. 1.]



```python
X_test_scaled = (X_test - min_on_training) / range_on_training
svc = SVC()
svc.fit(X_train_scaled, y_train)

print("Accuracy on training set: {:.3f}".format(svc.score(X_train_scaled, y_train)))
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))
```

Accuracy on training set: 0.984
Accuracy on test set: 0.972

数据缩放的作用很大！实际上模型现在处于欠拟合的状态，因为训练集和测试集的性能非常接近，但还没有接近100%的精度。从这里开始，我们可以尝试增大C或gamma来拟合更为复杂的模型。例如：

```python
svc = SVC(C=1000)
svc.fit(X_train_scaled, y_train)

print("Accuracy on training set: {:.3f}".format(svc.score(X_train_scaled, y_train)))
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))
```
> Accuracy on training set: 1.000
> Accuracy on test set: 0.958

____

## 参考
- 机器学习算法原理笔记（一）—— SVM支持向量机：[https://www.jianshu.com/p/341c5edd85f5](https://www.jianshu.com/p/341c5edd85f5)
- 【机器学习】支持向量机：SVM,support vector machines：[https://www.jianshu.com/p/9f6e685d04f6](https://www.jianshu.com/p/9f6e685d04f6)
- SVM支持向量机：[https://www.cnblogs.com/steven-yang/p/5658362.html](https://www.cnblogs.com/steven-yang/p/5658362.html)
- 解密SVM系列（三）：SMO算法原理与实战求解：[https://blog.csdn.net/on2way/article/details/47730367](https://blog.csdn.net/on2way/article/details/47730367)
- 机器学习实战之SVM：[https://www.cnblogs.com/zy230530/p/6901277.html](https://www.cnblogs.com/zy230530/p/6901277.html)
- 支持向量机原理详解(八): 多类分类SVM：[https://zhuanlan.zhihu.com/p/66933242](https://zhuanlan.zhihu.com/p/66933242)
- 支持向量机原理详解(一): 间隔最大化，支持向量：[https://zhuanlan.zhihu.com/p/60743894](https://zhuanlan.zhihu.com/p/60743894)