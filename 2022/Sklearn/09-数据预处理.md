
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面写了一篇[数据预处理之标准化方法](https://blog.csdn.net/xq151750111/article/details/124461551)，今天来介绍使用sklearn库进行数据预处理的方法。

当我们拿到一批原始的数据

1. 首先要明确有多少特征，哪些是连续的，哪些是类别的。
2. 检查有没有缺失值，对缺失的特征选择恰当方式进行弥补，使数据完整。
3. 对连续的数值型特征进行标准化，使得均值为0，方差为1。
4. 对类别型的特征进行one-hot编码。
5. 将需要转换成类别型数据的连续型数据进行二值化。
6. 为防止过拟合或者其他原因，选择是否要将数据进行正则化，或者先进行特征选择或者降维。
7. 在对数据进行初探之后发现效果不佳，可以尝试使用多项式方法，寻找非线性的关系。
8. 根据实际问题分析是否需要对特征进行相应的函数转换。

在图1中，第一张图显示的是一个模拟的有两个特征的二分类数据集。第一个特征（x轴）位于10到15之间。第二个特征（y轴）大约位于1到9之间。

接下来的4 张图展示了4 种数据变换方法，都生成了更加标准的范围。scikit-learn中的`StandardScaler`确保每个特征的平均值为0、方差为1，使所有特征都位于同一量级。但这种缩放不能保证特征任何特定的最大值和最小值。`RobustScaler`的工作原理与`StandardScaler`类似，确保每个特征的统计属性都位于同一范围。但RobustScaler使用的是中位数和四分位数1，而不是平均值和方差。这样`RobustScaler`会忽略与其他点有很大不同的数据点（比如测量误差）。这些与众不同的数据点也叫异常值（outlier），可能会给其他缩放方法造成麻烦。

与之相反，MinMaxScaler移动数据，使所有特征都刚好位于0到1之间。对于二维数据集来说，所有的数据都包含在x 轴0 到1 与y 轴0 到1 组成的矩形中。

最后，Normalizer用到一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于1。换句话说，它将一个数据点投射到半径为1 的圆上（对于更高维度的情况，是球面）。这意味着每个数据点的缩放比例都不相同（乘以其长度的倒数）。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/fae3ecd6fb554c5c9ca6fcc2189596f1.png#pic_center" width=48%> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图1 对数据集缩放和预处理的各种方法</div> </center>


## 1 数据标准化

## 1.1 标准化：去均值，方差归一化

Standardization标准化：将特征数据的分布调整成标准正态分布，也叫高斯分布，也就是使得数据的均值维0，方差为1。

标准化的原因在于如果有些特征的方差过大，则会主导目标函数从而使参数估计器无法正确地去学习其他特征。

标准化的过程为两步：去均值的中心化（均值变为0）；方差的归一化（方差变为1）。

在 `sklearn.preprocessing`中提供了一个 `scale()`的方法，可以实现以上功能

MaxAbsScaler()
原理与上面的很像，只是数据会被规模化到[-1,1]之间。也就是特征中，所有数据都会除以最大值。这个方法对那些已经中心化均值维0或者稀疏的数据有意义。

### 1.2 规模化稀疏数据

如果对稀疏数据进行去均值的中心化就会破坏稀疏的数据结构。虽然如此，我们也可以找到方法去对稀疏的输入数据进行转换，特别是那些特征之间的数据规模不一样的数据。

MaxAbsScaler和maxabs_scale这两个方法是专门为稀疏数据的规模化所设计的。

## 1.3 规模化有异常值的数据

如果你的数据有许多异常值，那么使用数据的均值与方差去做标准化就不行了。

在这里，你可以使用robust_scale和RobustScaler这两个方法。它会根据中位数或者四分位数去中心化数据。

### 1.4 正则化Normalization

正则化是将样本在向量空间模型上的一个转换，经常被使用在分类与聚类中。

函数normalize 提供了一个快速有简单的方式在一个单向量上来实现这正则化的功能。正则化有l1,l2等，这些都可以用上：

### 1.5 二值化–特征的二值化

特征的二值化是指将数值型的特征数据转换成布尔类型的值。可以使用实用类Binarizer。

## 2 为类别特征编码




## 3 弥补缺失数据



## 4 创建多项式特征

### 1.4 sklearn.preprocessing.scale()

函数原型：

```python
sklearn.preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=True)
```

作用：数据标准化
公式：(X-X_mean)/X_std 计算时对每个属性/每列分别进行。
原理：将数据按其属性（按列进行）减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1。

<table><thead><tr><th>参数名称</th><th>参数意义</th><th>默认值</th></tr></thead><tbody><tr><td>X</td><td>待处理的数组</td><td>默认</td></tr><tr><td>axis</td><td>处理哪个维度，0表示处理横向的数据（行）， 1表示处理纵向的数据（列）</td><td>0</td></tr><tr><td>with_mean</td><td>true是表示使均值为0</td><td>True</td></tr><tr><td>with_std</td><td>true表示使标准差为1</td><td>True</td></tr></tbody></table>

例子：

```python
from sklearn import preprocessing
import numpy as np

if __name__ == '__main__':

    X = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])
    X_scaled = preprocessing.scale(X, axis=0)
    print(X_scaled)
    # scaled之后的数据零均值，单位方差
    print(X_scaled.mean(axis=0))  # column mean: array([ 0.,  0.,  0.])
    print(X_scaled.std(axis=0))  # column standard deviation: array([ 1.,  1.,  1.])
```




## 5 案例分析
首先加载数据集并将其分为训练集和测试集（我们需要分开的训练集和数据集来对预处理后构建的监督模型进行评估）：
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)

print(X_train.shape)            # (426, 30)
print(X_test.shape)             # (143, 30)
```
导入实现预处理的类，然后将其实例化，接着，使用fit方法拟合缩放器（scaler），并将其应用于训练数据。对于MinMaxScaler来说，fit方法计算训练集中每个特征的最大值和最小值。在对缩放器调用fit 时只提供了X_train，而不用y_train：
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X_train)         # MinMaxScaler(copy=True, feature_range=(0, 1))
```
在scikit-learn中，每当模型返回数据的一种新表示时，都可以使用transform方法：

```python
# 变换数据
X_train_scaled = scaler.transform(X_train)
# 在缩放之前和之后分别打印数据集属性
print("transformed shape: {}".format(X_train_scaled.shape))
print("per-feature minimum before scaling: \n {}".format(X_train.min(axis=0)))
print("per-feature maximum before scaling: \n {}".format(X_train.max(axis=0)))
print("per-feature maximum after scaling: \n {}".format(X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling: \n {}".format(X_train_scaled.max(axis=0)))
```
<img src ="https://img-blog.csdnimg.cn/ada74eac67b6468ebf25ee2207d0d37f.png#pic_center" width = 48%>

变换后的数据形状与原始数据相同，特征只是发生了移动和缩放。你可以看到，现在所有特征都位于0到1之间，这也符合我们的预期。

为了将SVM 应用到缩放后的数据上，还需要对测试集进行变换。这可以通过对X_test调用transform方法来完成：

```python
# 对测试数据进行变换
X_test_scaled = scaler.transform(X_test)
# 在缩放之后打印测试数据集
print("per-feature maximum after scaling: \n {}".format(X_test_scaled.min(axis=0)))
print("per-feature maximum after scaling: \n {}".format(X_test_scaled.max(axis=0)))
```
<img src ="https://img-blog.csdnimg.cn/55cc2ace717a43cd86989fd57bc4b37c.png#pic_center" width = 48%>

可以发现，对测试集缩放后的最大值和最小值不是1 和0，这或许有些出乎意料。有些特征甚至在0~1的范围之外！对此的解释是，MinMaxScaler（以及其他所有缩放器）总是对训练集和测试集应用完全相同的变换。也就是说，transform 方法总是减去训练集的最小值，然后除以训练集的范围，而这两个值可能与测试集的最小值和范围并不相同。

如果我们使用测试集的最小值和范围，下面这个例子展示了会发生什么：
```python
from sklearn.datasets import make_blobs
# 构造数据
X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)
# 将其分为训练集和测试集
X_train, X_test = train_test_split(X, random_state=5, test_size=.1)
# 绘制训练集和测试集
fig, axes = plt.subplots(1, 3, figsize=(13, 4))
axes[0].scatter(X_train[:, 0], X_train[:, 1],
c=mglearn.cm2(0), label="Training set", s=60)
axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',
c=mglearn.cm2(1), label="Test set", s=60)
axes[0].legend(loc='upper left')
axes[0].set_title("Original Data")
# 利用MinMaxScaler缩放数据
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# 将正确缩放的数据可视化
axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],
c=mglearn.cm2(0), label="Training set", s=60)
axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',
c=mglearn.cm2(1), label="Test set", s=60)
axes[1].set_title("Scaled Data")
# 单独对测试集进行缩放
# 使得测试集的最小值为0，最大值为1
# 千万不要这么做！这里只是为了举例
test_scaler = MinMaxScaler()
test_scaler.fit(X_test)
X_test_scaled_badly = test_scaler.transform(X_test)
# 将错误缩放的数据可视化
axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],
c=mglearn.cm2(0), label="training set", s=60)
axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],
marker='^', c=mglearn.cm2(1), label="test set", s=60)
axes[2].set_title("Improperly Scaled Data")
for ax in axes:
    ax.set_xlabel("Feature 0")
    ax.set_ylabel("Feature 1")
```
<img src ="https://img-blog.csdnimg.cn/b74c6210969c44ad889bab19fe7e7618.png#pic_center" width = 48%>

第一张图是未缩放的二维数据集，其中训练集用圆形表示，测试集用三角形表示。第二张图中是同样的数据，但使用MinMaxScaler缩放。这里我们调用fit作用在训练集上，然后调用transform作用在训练集和测试集上。你可以发现，第二张图中的数据集看起来与第一张图中的完全相同，只是坐标轴刻度发生了变化。现在所有特征都位于0到1之间。你还可以发现，测试数据（三角形）的特征最大值和最小值并不是1和0。


第三张图展示了如果我们对训练集和测试集分别进行缩放会发生什么。在这种情况下，对训练集和测试集而言，特征的最大值和最小值都是1和0。但现在数据集看起来不一样。测试集相对训练集的移动不一致，因为它们分别做了不同的缩放。我们随意改变了数据的排列。这显然不是我们想要做的事情。

为了对比，我们再次在原始数据上拟合SVC：

```python
from sklearn.svm import SVC

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

svm = SVC(C=100)
svm.fit(X_train, y_train)
print("Test set accuracy: {:.2f}".format(svm.score(X_test, y_test)))        # Test set accuracy: 0.63
```

下面先用MinMaxScaler对数据进行缩放，然后再拟合SVC：

```python
# 使用0-1缩放进行预处理
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# 缩放后的训练数据上学习SVM
svm.fit(X_train_scaled, y_train)

# 缩放后的测试集上计算分数
print("Scaled test set accurcay: {:.2f}".format(svm.score(X_test_scaled, y_test)))      # Scaled test set accurcay: 0.97
```

正如我们上面所见，数据缩放的作用非常显著。虽然数据缩放不涉及任何复杂的数学，但良好的做法仍然是使用scikit-learn 提供的缩放机制，而不是自己重新实现它们，因为即使在这些简单的计算中也容易犯错。

也可以通过改变使用的类将一种预处理算法轻松替换成另一种，因为所有的预处理类都具有相同的接口，都包含fit和transform方法：

```python
from sklearn.preprocessing import StandardScaler

# 利用零均值和单位方差的缩放方法进行预处理
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# 缩放后的训练数据上学习SVM
svm.fit(X_train_scaled, y_train)

# 缩放后的测试集上计算分数
print("Scaled test set accurcay: {:.2f}".format(svm.score(X_test_scaled, y_test)))  # Scaled test set accurcay: 0.96
```

_____

## 参考

预处理数据的方法总结（使用sklearn-preprocessing）：[https://blog.csdn.net/weixin_40807247/article/details/82793220](https://blog.csdn.net/weixin_40807247/article/details/82793220)
