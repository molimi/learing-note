-   NMF 将数据分解成非负分量，并且系数也是非负的。NMF 的分量更容易解释。
    -   两个分量的 NMF，所有数据点都可以写成这两个分量的正数组合
    -   一个分量的 NMF，分量会指向数据的平均值
    -   如果有足够多的分量重建数据(分量个数与特征个数相同)，算法会选择指向数据极值的方向
        -   分量数目的改变会导致分量的改变
        -   所有分量的地位平等
        -   初始化时的不同的随机种子会产生不同的分量结果
-   NMF 设定的分量个数不一样，产生的分量的内容也不一样。
-   NMF 所有分量的地位平等。
-   NMF 比较适合于具有叠加结构的数据，包括：音频、基因表达和文本数据。


## 1 非负矩阵分解(NFM)
NMF(Non-negative matrix factorization)，即对于任意给定的一个非负矩阵$\pmb{V}$，其能够寻找到一个非负矩阵$\pmb{W}$和一个非负矩阵$\pmb{H}$，满足条件$\pmb{V=W*H}$,从而将一个非负的矩阵分解为左右两个非负矩阵的乘积。**其中，$\pmb{V}$矩阵中每一列代表一个观测(observation)，每一行代表一个特征(feature)；$\pmb{W}$矩阵称为基矩阵，$\pmb{H}$矩阵称为系数矩阵或权重矩阵。这时用系数矩阵$\pmb{H}$代替原始矩阵，就可以实现对原始矩阵进行降维，得到数据特征的降维矩阵，从而减少存储空间。** 过程如下图所示：

<img src ="https://img-blog.csdnimg.cn/61ead0e6cee0434880c36b9898172d8e.png#pic_center" width = 48%>




### 1.1 将 NMF 应用于人脸图像
与使用PCA不同，我们需要保证数据是正的。这说明数据相对于原点(0, 0)的位置实际上对NMF很重要。因此，你可以将提取出来的非负分量看作是从(0, 0)到数据的方向。
下面的例子给出了NMF 在二维玩具数据上的结果：
<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/cf127efea65649bb93eccdb25d77b843.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图1 两个分量的非负矩阵分解(左)和一个分量的非负矩阵分解(右)找到的分量</div> </center>


首先，我们来观察分量个数如何影响NMF 重建数据的好坏：

```python
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split

# 导入人脸图像
people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
image_shape = people.images[0].shape

# 每类最多取50张
mask = np.zeros(people.target.shape, dtype=np.bool)
for target in np.unique(people.target):
    mask[np.where(people.target == target)[0][:50]] = 1

X_people = people.data[mask]
y_people = people.target[mask]

# 将灰度值缩放到0到1之间，而不是在0到255之间
# 以得到更好的数据稳定性
X_people = X_people / 255


# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_people, y_people, stratify=y_people, random_state=0)

mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/f48e8254f7ea473ab323d81609fbab64.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图2 利用越来越多分量的NMF重建三张人脸图像</div> </center>


反向变换的数据质量与使用PCA时类似，但要稍差一些。这是符合预期的，因为PCA找到的是重建的最佳方向。NMF通常并不用于对数据进行重建或编码，而是用于在数据中寻找有趣的模式。

我们尝试仅提取一部分分量(比如15个)，初步观察一下数据。
```python
from sklearn.decomposition import NMF
nmf = NMF(n_components=15, random_state=0)
nmf.fit(X_train)
X_train_nmf = nmf.transform(X_train)
X_test_nmf = nmf.transform(X_test)

fig, axes = plt.subplots(3, 5, figsize=(15, 12),
                         subplot_kw={'xticks': (), 'yticks': ()})
for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):
    ax.imshow(component.reshape(image_shape))
    ax.set_title("{}. component".format(i))
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/67b0975cf72d45148cf3b8efa11747b2.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图3 使用15个分量的NMF在人脸数据集上找到的分量</div> </center>


你可以清楚地看到，分量3（component 3）显示了稍微向右转动的人脸，而分量7（component 7）则显示了稍微向左转动的人脸。我们来看一下这两个分量特别大的那些图像，


```python
compn = 3
# 第3个分量的系数较大的人脸
inds = np.argsort(X_train_nmf[:, compn])[::-1]
fig, axes = plt.subplots(2, 5, figsize=(15, 8),
                         subplot_kw={'xticks': (), 'yticks': ()})
fig.suptitle("Large component 3")
for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):
    ax.imshow(X_train[ind].reshape(image_shape))
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/aee8b59fa7fa4e1eaf05d395acec603b.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图4 分量3系数较大的人脸</div> </center>


```python
compn = 7
# 第7个分量的系数较大的人脸
inds = np.argsort(X_train_nmf[:, compn])[::-1]
fig.suptitle("Large component 7")
fig, axes = plt.subplots(2, 5, figsize=(15, 8),
                         subplot_kw={'xticks': (), 'yticks': ()})
for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):
    ax.imshow(X_train[ind].reshape(image_shape))
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/1ca07f7da5014a29a485d8e504aca0de.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图5 分量7系数较大的人脸</div> </center>

正如所料，分量3系数较大的人脸都是向右看的人脸（图4），而分量7系数较大的人脸都向左看（图5）。如前所述，提取这样的模式最适合于具有叠加结构的数据，包括音频、基因表达和文本数据。我们通过一个模拟数据的例子来看一下这种用法。




### 1.2 盲源信号分离
假设我们对一个信号感兴趣，它是三个不同信号源合成的：
```python
S = mglearn.datasets.make_signals()
plt.figure(figsize=(6, 1))
plt.plot(S, '-')
plt.xlabel('Time')
plt.ylabel('Signal')
```
<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/892334fb822b4c91896b60b9821db3e2.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图6 原始信号源</div> </center>

不幸的是，我们无法观测到原始信号，只能观测到三个信号的叠加混合。我们想要将混合信号分解为原始分量。假设我们有许多种不同的方法来观测混合信号（比如有100台测量装置），每种方法都为我们提供了一系列测量结果。

```python
# 将数据混合成100维的状态
A = np.random.RandomState(0).uniform(size=(100, 3))
X = np.dot(S, A.T)
print("Shape of measurements: {}".format(X.shape))      # Shape of measurements: (2000, 100)
```
我们可以用NMF和PCA来还原这三个信号：
```python
from sklearn.decomposition import PCA 
pca = PCA(n_components=3)
H = pca.fit_transform(X)
```

给出了NMF和PCA发现的信号活动：

```python
models = [X, S, S_, H]
names = ['Observations (first three measurements)',
         'True sources',
         'NMF recovered signals',
         'PCA recovered signals']

fig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={'hspace': .5}, subplot_kw={'xticks': (), 'yticks': ()})

for model, name, ax in zip(models, names, axes):
    ax.set_title(name)
    ax.plot(model[:, :3], '-')
```
<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/93038d2f1e2843a489aef81bccf7be4a.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图7 利用NMF和PCA还原混合信号源</div> </center>

图中包含来自X的100次测量中的3次，用于参考。可以看到，NMF在发现原始信号源时得到了不错的结果，而PCA则失败了，仅使用第一个成分来解释数据中的大部分变化。要记住，NMF生成的分量是没有顺序的。在这个例子中，NMF分量的顺序与原始信号完全相同（参见三条曲线的颜色），但这纯属偶然。


## 2 用t-SNE进行流形学习

流形学习算法主要用于可视化，因此很少用来生成两个以上的新特征。其中一些算法（包括t-SNE）计算训练数据的一种新表示，但不允许变换新数据。这意味着这些算法不能用于测试集：更确切地说，它们只能变换用于训练的数据。流形学习对探索性数据分析是很有用的，但如果最终目标是监督学习的话，则很少使用。t-SNE背后的思想是找到数据的一个二维表示，尽可能地保持数据点之间的距离。t-SNE首先给出每个数据点的随机二维表示，然后尝试让在原始特征空间中距离较近的点更加靠近，原始特征空间中相距较远的点更加远离。t-SNE 重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些表示哪些点比较靠近的信息。


我们将对scikit-learn包含的一个手写数字数据集2应用t-SNE流形学习算法。在这个数据集中，每个数据点都是0到9之间手写数字的一张8×8灰度图像。

```python
from sklearn.datasets import load_digits
digits = load_digits()
print("手写数字数据集的形状= {}".format(digits.data.shape))  # (1797, 64)
print("手写数字数据集中图片的形状= {}".format(digits.images.shape))  # (1797, 8, 8)

fig, axes = plt.subplots(2, 5, figsize=(10, 5),subplot_kw={'xticks':(), 'yticks': ()})
for ax, img in zip(axes.ravel(), digits.images):
    ax.imshow(img)
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/5b2fc89d77514b68ae2e5dd702f742ce.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图8 digits数据集的示例图像</div> </center>


我们用PCA将降到二维的数据可视化。我们对前两个主成分作图，并按类别对数据点着色。

```python
# 构建一个 PCA 模型
pca = PCA(n_components=2)
pca.fit(digits.data)
# 将 digits 数据变换到前两个主成分的方向上
digits_pca = pca.transform(digits.data)
colors = ["#476A2A", "#7851B8", "#BD3430", "#4A2D4E", "#875525",
          "#A83683", "#4E655E", "#853541", "#3A3120", "#535D8E"]
plt.figure(figsize=(10, 10))
plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())
plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())
for i in range(len(digits.data)):
    # 将数据绘制成文本图(代替散点图)
    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),
             color = colors[digits.target[i]],
             fontdict={'weight': 'bold', 'size': 9})
plt.xlabel("第一个主成分")
plt.ylabel("第二个主成分")
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/9a659e13dcbf4fcfb03bdc11cd3559d3.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图9 利用前两个主成分绘制digits 数据集的散点图</div> </center>

实际上，这里我们用每个类别对应的数字作为符号来显示每个类别的位置。利用前两个主成分可以将数字0、6和4相对较好地分开，尽管仍有重叠。大部分其他数字都大量重叠在一起。

我们将t-SNE应用于同一个数据集，并对结果进行比较。由于t-SNE不支持变换新数据，所以TSNE类没有transform方法。我们可以调用fit_transform方法来代替，它会构建模型并立刻返回变换后的数据：

```python
from sklearn.manifold import TSNE
tsne = TSNE(random_state=42)
# 使用 fit_transform() 代替 fit(), 因为 TSNE 没有 transform()
digits_tsne = tsne.fit_transform(digits.data)
```
```python
plt.figure(figsize=(10, 10))
plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)
plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)
for i in range(len(digits.data)):
    # 将数据绘制成文本图(代替散点图)
    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),
             color = colors[digits.target[i]],
             fontdict={'weight': 'bold', 'size': 9})
plt.xlabel("t-SNE feature 0")
plt.ylabel("t-SNE feature 1")
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/624257022f36488c9791b175707a538f.png#pic_center" width=48%> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图10 利用t-SNE找到的两个分量绘制digits数据集的散点图</div> </center>

t-SNE的结果非常棒。所有类别都被明确分开。数字1和9被分成几块，但大多数类别都形成一个密集的组。要记住，这种方法并不知道类别标签：它完全是无监督的。但它能够找到数据的一种二维表示，仅根据原始空间中数据点之间的靠近程度就能够将各个类别明确分开。

____


## 参考

- Nimfa is a Python library for nonnegative matrix factorization：[http://nimfa.biolab.si/](http://nimfa.biolab.si/)
- Non-negative matrix factorization：[https://en.wikipedia.org/wiki/Non-negative_matrix_factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)