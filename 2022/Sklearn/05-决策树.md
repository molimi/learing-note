决策树是广泛用于分类和回归任务的模型。本质上，它从一层层的`if/else`问题中进行学习，并得出结论。决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、C4.5和CART。

想象一下，你想要区分下面这四种动物：熊、鹰、企鹅和海豚。你的目标是通过提出尽可能少的if/else 问题来得到正确答案。你可能首先会问：这种动物有没有羽毛，这个问题会将可能的动物减少到只有两种。如果答案是“有”，你可以问下一个问题，帮你区分鹰和企鹅。例如，你可以问这种动物会不会飞。如果这种动物没有羽毛，那么可能是海豚或熊，所以你需要问一个问题来区分这两种动物——比如问这种动物有没有鳍。这一系列问题可以表示为一棵决策树，如下图所示。

```python
import mglearn
import os
from graphviz import Digraph

# os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz/bin'

mglearn.plots.plot_animal_tree()
```

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src ="https://img-blog.csdnimg.cn/7f7a579d1aed4069a34ef40031c7247b.png#pic_center" width = 48%><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图1 区分几种动物的决策树</div> </center>

既然要做决策，需要决定的就是从哪个维度（特征）来做决策，例如前面例子中的会不会飞、有没有羽毛等。在机器学习中我们需要一个量化的指标来确定使用的特征更加合适，即使用该特征划分后，得到的子集合的“纯度”更高。这时引入三种指标——信息增益（Information Gain）、基尼指数（Gini Index）、均方差（MSE）来解决前面说的问题。


## 1 量化指标
### 1.1 信息熵

熵度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量X的熵的表达式如下：
$$H(p)=H(X)=-\sum_{i=1}^{n}p_i\log p_i$$

注意：**熵只与$X$的分布有关，与$X$取值无关**。定义$0\log0=0$，熵是非负的。

### 1.2 条件熵

随机变量$(X,Y)$的联合概率分布为

$P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\dots ,n;j=1,2,\dots ,m$


两个变量$X$和$Y$的联合熵表达式如下：
$$H(X,Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i,y_i)$$

条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，类似于条件概率，它度量了在Y知道X以后剩下的不确定性。表达式如下：
$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$
其中$p_i=P(X=x_i),i=1,2,\dots ,n$

$H(X)$度量了$X$的不确定性，条件熵$H(X|Y)$度量了我们在知道$Y$以后$X$剩下的不确定性，那么$H(X)-H(X|Y)$呢？从上面的描述大家可以看出，它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为$I(X,Y)$。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。


当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。就是从已知的数据计算得到的结果。

### 1.3 信息增益
特征$A$对训练数据集$D$的信息增益$g(D|A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差
$$g(D,A)=H(D)-H(D|A)$$

熵与条件熵的差称为互信息.

决策树中的信息增益等价于训练数据集中的类与特征的互信息。

考虑ID这种特征，本身是唯一的。按照ID做划分，得到的经验条件熵为0，会得到最大的信息增益。所以，按照信息增益的准则来选择特征，可能会倾向于取值比较多的特征。

> 那么，信息增益的物理意义是什么呢？
> 如果以概率$P(x)$为横坐标，以信息熵(Entropy)为纵坐标，把信息熵和概率的函数关系${Entropy=-P(x)log_2P(x)}$在二维坐标系上画出来就可以看出(标有“Entropy”的曲线)，当概率$P(x)$越接近0或越接近1时，信息熵的值越小，其不确定性越小，即数据越“纯”。比如说当概率为1时，数据是最“纯净的”，已经消除了不确定性，其信息熵为0。我们在选择特征的时候，选择信息增益最大的特征，在物理意义上就是让数据尽量往更纯净的方向上变换。因此，信息增益是用来衡量数据变得更有序、更纯净的程度的指标。

<img src ="https://img-blog.csdnimg.cn/17fdc926477c41f8b867cda8c0ab4e4d.png#pic_center" width = 48%>


### 1.4 信息增益比
引入一个信息增益比的变量$g_R(D,A)$，它是信息增益和特征熵的比值。表达式如下：
$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

对于特征熵$H_A(D)$, 表达式如下：
$$H_A(D)=-\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}$$


### 1.5 基尼指数
在分类问题中，假设有$K$个类别，第$k$个类别的概率为$p_k$, 则基尼系数的表达式为：
$$Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2$$

如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是$p$，则基尼系数的表达式为：

对于个给定的样本$D$，假设有$K$个类别, 第$k$个类别的数量为$C_k$，则样本$D$的基尼系数表达式为：
$$Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2$$

特别的，对于样本$D$，如果根据特征$A$的某个值$a$，把$D$分成$D_1$和$D_2$两部分，则在特征$A$的条件下，$D$的基尼系数表达式为：

$$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)$$

大家可以比较下基尼系数表达式和熵模型的表达式，二次运算是不是比对数简单很多？尤其是二类分类的计算，更加简单。但是简单归简单，和熵模型的度量方式比，基尼系数对应的误差有多大呢？对于二类分类，基尼系数和熵之半的曲线如下：
<img src ="https://img-blog.csdnimg.cn/7839389033ba4fc9b66e7156af3be5e8.png#pic_center" width = 48%>

从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。

### 1.6 代码实现

```python
from math import log
# 熵
def calc_ent(datasets):
    data_length = len(datasets)
    label_count = {}        # 存储每个label出现次数
    for feat_vec in datasets:
        label = feat_vec[-1]
        label_count[label] = label_count.get(label, 0) + 1

    ent = -sum([(p / data_length) * log(p / data_length, 2)
                for p in label_count.values()])     # 计算经验熵
    return ent


# 经验条件熵
def cond_ent(datasets, axis=0):
    data_length = len(datasets)
    feature_sets = {}
    for i in range(data_length):
        feature = datasets[i][axis]
        if feature not in feature_sets:
            feature_sets[feature] = []
        feature_sets[feature].append(datasets[i])   # 根据特征分割数据集
    cond_ent = sum(
        [(len(p) / data_length) * calc_ent(p) for p in feature_sets.values()])      # 计算经验条件熵
    return cond_ent


# 信息增益
def info_gain(ent, cond_ent):
    return ent - cond_ent


def info_gain_train(datasets, labels):
    count = len(datasets[0]) - 1
    ent = calc_ent(datasets)
    best_feature = []
    for c in range(count):
        c_info_gain = info_gain(ent, cond_ent(datasets, axis=c))    # 计算每个特征的信息增益
        best_feature.append((c, c_info_gain))
        print('特征({}) - info_gain - {:.3f}'.format(labels[c], c_info_gain))
    # 比较大小
    best_ = max(best_feature, key=lambda x: x[-1])
    return '特征({})的信息增益最大，选择为根节点特征'.format(labels[best_[0]])
```


前面信息论部分简单介绍过熵、条件熵、KL散度、交叉熵，具体了解请阅读[统计篇（二）-- 概率论、随机过程、信息论知识汇总](https://blog.csdn.net/xq151750111/article/details/127495987?spm=1001.2014.3001.5501)和[高数篇（四）-- 互信息概述与matlab实现](https://blog.csdn.net/xq151750111/article/details/123531201)

## 2 算法原理

### 2.1 ID3算法
**1. 信息增益**
> 输入：训练数据集$D$和特征$A$
>
> 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$
>
> 1. 数据集$D$的经验熵$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$
> 1.  特征$A$对数据集$D$的经验条件熵$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$
> 1. 信息增益$g(D,A)=H(D)-H(D|A)$

**2. 算法流程**

> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
> 1. 如果$A$是空集，置$T$为单节点树，实例数最多的类作为该节点类标记，返回$T$
> 1. 计算$g$, 选择信息增益最大的特征$A_g$
> 1. 如果$A_g$的信息增益小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 1. $A_g$划分若干非空子集$D_i$，
> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$


**3. 缺点**
- ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
- ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如何校正这个问题呢？
- ID3算法对于缺失值的情况没有做考虑
- 没有考虑过拟合的问题

**4. 代码实现**
```python
# 定义节点类 二叉树
class Node:
    def __init__(self, root=True, label=None, feature_name=None, feature=None):
        self.root = root
        self.label = label
        self.feature_name = feature_name
        self.feature = feature
        self.tree = {}
        self.result = {
            'label:': self.label,
            'feature': self.feature,
            'tree': self.tree
        }

    def __repr__(self):
        return '{}'.format(self.result)

    def add_node(self, val, node):
        self.tree[val] = node

    def predict(self, features):
        if self.root is True:
            return self.label
        return self.tree[features[self.feature]].predict(features)


class DTree:
    def __init__(self, epsilon=0.1):
        self.epsilon = epsilon
        self._tree = {}

    # 熵
    @staticmethod
    def calc_ent(datasets):
        data_length = len(datasets)
        label_count = {}
        for i in range(data_length):
            label = datasets[i][-1]
            if label not in label_count:
                label_count[label] = 0
            label_count[label] += 1
        ent = -sum([(p / data_length) * log(p / data_length, 2)
                    for p in label_count.values()])
        return ent

    # 经验条件熵
    def cond_ent(self, datasets, axis=0):
        data_length = len(datasets)
        feature_sets = {}
        for i in range(data_length):
            feature = datasets[i][axis]
            if feature not in feature_sets:
                feature_sets[feature] = []
            feature_sets[feature].append(datasets[i])
        cond_ent = sum([(len(p) / data_length) * self.calc_ent(p)
                        for p in feature_sets.values()])
        return cond_ent

    # 信息增益
    @staticmethod
    def info_gain(ent, cond_ent):
        return ent - cond_ent

    def info_gain_train(self, datasets):
        count = len(datasets[0]) - 1
        ent = self.calc_ent(datasets)
        best_feature = []
        for c in range(count):
            c_info_gain = self.info_gain(ent, self.cond_ent(datasets, axis=c))
            best_feature.append((c, c_info_gain))
        # 比较大小
        best_ = max(best_feature, key=lambda x: x[-1])
        return best_

    def train(self, train_data):
        """
        input:数据集D(DataFrame格式)，特征集A，阈值eta
        output:决策树T
        """
        _, y_train, features = train_data.iloc[:, : -1], train_data.iloc[:, -1], train_data.columns[: -1]
        # 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T
        if len(y_train.value_counts()) == 1:
            return Node(root=True, label=y_train.iloc[0])

        # 2, 若A为空，则T为单节点树，将D中实例树最大的类Ck作为该节点的类标记，返回T
        if len(features) == 0:
            return Node(
                root=True,
                label=y_train.value_counts().sort_values(
                    ascending=False).index[0])

        # 3,计算最大信息增益 同5.1,Ag为信息增益最大的特征
        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))
        max_feature_name = features[max_feature]

        # 4,Ag的信息增益小于阈值eta,则置T为单节点树，并将D中是实例数最大的类Ck作为该节点的类标记，返回T
        if max_info_gain < self.epsilon:
            return Node(
                root=True,
                label=y_train.value_counts().sort_values(
                    ascending=False).index[0])

        # 5,构建Ag子集
        node_tree = Node(
            root=False, feature_name=max_feature_name, feature=max_feature)

        feature_list = train_data[max_feature_name].value_counts().index
        for f in feature_list:
            sub_train_df = train_data.loc[train_data[max_feature_name] ==
                                          f].drop([max_feature_name], axis=1)

            # 6, 递归生成树
            sub_tree = self.train(sub_train_df)
            node_tree.add_node(f, sub_tree)

        # pprint.pprint(node_tree.tree)
        return node_tree

    def fit(self, train_data):
        self._tree = self.train(train_data)
        return self._tree

    def predict(self, X_test):
        return self._tree.predict(X_test)
```


### 2.2 C4.5算法

**1. 原理介绍**
针对ID3算法的不足，昆兰在C4.5算法中改进了上述4个问题。对于第一个问题，不能处理连续特征，C4.5的思路是将连续的特征离散化。比如$m$个样本的连续特征A有$m$个，从小到大排列为$a_1, a_2, \cdots, a_m$，则C4.5取相邻两样本值的平均数，一共取得$m-1$个划分点，其中第$i$个划分点$T_i$表示为：$T_i=\frac{a_i+a_{i+1}}{2}$。对于这$m-1$个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$,则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

对于第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题。这里引入信息增益比，特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。

对于第三个缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。

对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

对于第4个问题，C4.5引入了正则化系数进行初步的剪枝。这部分内容在后面CART的时候会详细讨论。

**2. 算法流程**
> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
> 1. 如果$A$是空集, 置$T$为单节点树，实例数最多的作为该节点类标记，返回$T$
> 1. 计算$g$, 选择**信息增益比**最大的特征$A_g$
> 1. 如果$A_g$的**信息增益比**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 1. $A_g$划分若干非空子集$D_i$，
> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$
> ID3和C4.5在生成上，差异只在准则的差异。


**3. 缺点**
- 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。

- C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
- C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
- C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。

### 2.3 CART分类树算法
**1. 算法原理**
前面讨论了决策树里的ID3算法和C4.5算法的不足，接下来就看CART算法如何解决这些问题。在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5，都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。

CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。

对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。

具体的思路如下，比如$m$个样本的连续特征$A$有$m$个，从小到大排列为$a_1, a_2, \cdots, a_m$，则CART算法取相邻两样本值的平均数，一共取得$m-1$个划分点，其中第$i$个划分点$T_i$表示为：$Ti=\frac{a_i+a_{i+1}}{2}$。对于这$m-1$个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为$a_t$,则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。

回忆下ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。

**2. 算法流程**
> 输入：训练集D，基尼系数的阈值，样本个数阈值 
> 输出：决策树T
>
> 我们的算法从根节点开始，用训练集递归的建立CART树。
> 
> 1. 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。
> 1. 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。
> 1. 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第一节。缺失值的处理方法和上一小节的C4.5算法里描述的相同。
> 1. 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.
> 1. 对左右的子节点递归的调用1-4步，生成决策树。

对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。

### 2.4 CART回归树算法
**1. 算法原理**
如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：
- 连续值的处理方法不同
- 决策树建立后做预测的方式不同

对于连续值的处理，我们知道CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于回归模型，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：
$$\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]$$

其中，$c_1$为D1数据集的样本输出均值，$c_2$为D2数据集的样本输出均值。

对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。

**2. 算法流程**
> 输入：训练数据集$D$
> 输出：回归树$f(x)$
>
> 步骤：
>
> 1. 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，得到满足上面关系的$(j,s)$
>   $$
   \min\limits_{j,s}\left[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
>   $$
>
> 1. 用选定的$(j,s)$, 划分区域并决定相应的输出值
>  $$
   R_1(j,s)=\{x|x^{(j)}\leq s\}, R_2(j,s)=\{x|x^{(j)}> s\} \\
   \hat{c}_m= \frac{1}{N}\sum\limits_{x_i\in R_m(j,s)} y_j, x\in R_m, m=1,2
>   $$
>
> 1. 对两个子区域调用(1)(2)步骤， 直至满足停止条件
> 1. 将输入空间划分为$M$个区域$R_1, R_2,\dots,R_M$，生成决策树：
   $$
   f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)
   $$

### 2.5 正则化

当递归的生成决策树时，模型对训练数据的分类会非常准确，但是对未知的预测数据的表现并不理想，这就是所谓的过拟合的现象，这时可以同上一篇线性模型学习到的应对过拟合的解决方法一样，对模型进行正则化。

**1. 决策树的深度**
可以通过限制决策树的最大深度来达到对其正则化的效果，防止决策树过拟合。这时只需在算法步骤中加上一个用于记录当前递归下树深度的参数，当到达预设的最大深度时，不再生成新的子结点，将当前结点标记为样本中分类占比最大的分类并退出当前递归。

**2. 决策树的叶子结点大小**
另一个对决策树进行正则化的方法是限制叶子结点最少包含的样本数量，同样可以防止过拟合的现象。当结节包含的样本数，将当前结点标记为样本中分类占比最大的分类并退出当前递归。


**3. 决策树的剪枝**
还可以通过对决策树进行剪枝来防止其过拟合，将多余的子树剪断。剪枝的方法又分成两种，分别为预剪枝（prepruning）、后剪枝（post-pruning）。

（1）预剪枝
 顾名思义，预剪枝就是在生成决策树的时候就决定是否生成子结点，判断的方法为使用验证数据集比较生成子结点与不生成子结点的精度，当生成子结点的精度有提升，则生成子结点，反之则不生成子结点。【西瓜数据集】预剪枝决策树流程图，选择属性“脐部”来对训练集进行划分判别西瓜的好坏。

<img src ="https://img-blog.csdnimg.cn/0a00e847ac8e4eeab5f807e10c9d4a6d.png#pic_center" width = 48%>


（2）后剪枝
后剪枝则是先生成一个完整的决策树，然后再从叶子结点开始，同预剪枝一样的判断方法，当生成子结点的精度有提升，则保留子结点，反之则将子结点剪断。【西瓜数据集】后剪枝决策树流程图，选择属性“脐部”来对训练集进行划分判别西瓜的好坏。
<img src ="https://img-blog.csdnimg.cn/15eafac7df644e9e9098245e68c364eb.png#pic_center" width = 48%>


后剪枝决策树往往比预剪枝决策树保留更多的分支，一般情况下，后剪枝决策树的欠拟合风险较小，泛化能力往往优于预剪枝决策树。但后剪枝的训练时间比未剪枝和预剪枝都要长。


## 3 代码实现

### 3.1 构造决策树
**1. 构造决策树**
学习决策树，就是学习一系列if/else问题，使我们能够以最快的速度得到正确答案。在机器学习中，这些问题叫作测试（不要与测试集弄混，测试集是用来测试模型泛化性能的数据）。数据通常并不是像动物的例子那样具有二元特征（是/否）的形式，而是表示为连续特征，比如下图所示的二维数据集。用于连续数据的测试形式是：“特征i的值是否大于a？”

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src ="https://img-blog.csdnimg.cn/8ea438f72d334ab7a9f54d3fc4a9af0b.png#pic_center" width = 48%><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图2 用于构造决策树的two_moons数据集</div> </center>


为了构造决策树，算法搜遍所有可能的测试，找出对目标变量来说信息量最大的那一个。图3展示了选出的第一个测试。将数据集在x[1]=0.0596 处垂直划分可以得到最多信息，它在最大程度上将类别0 中的点与类别1中的点进行区分。顶结点（也叫根结点）表示整个数据集，包含属于类别0的50 个点和属于类别1的50个点。通过测试x[1] <=0.0596 的真假来对数据集进行划分，在图中表示为一条黑线。如果测试结果为真，那么将这个点分配给左结点，左结点里包含属于类别0的2个点和属于类别1的32个点。否则将这个点分配给右结点，右结点里包含属于类别0的48个点和属于类别1的18个点。这两个结点对应于图3中的顶部区域和底部区域。尽管第一次划分已经对两个类别做了很好的区分，但底部区域仍包含属于类别0的点，顶部区域也仍包含属于类别1的点。我们可以在两个区域中重复寻找最佳测试的过程，从而构建出更准确的模型。图4展示了信息量最大的下一次划分，这次划分是基于x[0] 做出的，分为左右两个区域。

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src ="https://img-blog.csdnimg.cn/c26b195828d44b4fa3ec45f470078623.png#pic_center" width = 48%><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图3 深度为1的树的决策边界（左）与相应的树（右）</div> </center>


<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src ="https://img-blog.csdnimg.cn/63997a8bc25e49a1a2aec10ce07c1213.png#pic_center" width = 48%><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图4 深度为2的树的决策边界（左）与相应的树（右）</div> </center>

这一递归过程生成一棵二元决策树，其中每个结点都包含一个测试。或者你可以将每个测
试看成沿着一条轴对当前数据进行划分。这是一种将算法看作分层划分的观点。由于每个
测试仅关注一个特征，所以划分后的区域边界始终与坐标轴平行。

对数据反复进行递归划分，直到划分后的每个区域（决策树的每个叶结点）只包含单一目
标值（单一类别或单一回归值）。如果树中某个叶结点所包含数据点的目标值都相同，那
么这个叶结点就是纯的（pure）。这个数据集的最终划分结果见图5。

<img src ="https://img-blog.csdnimg.cn/0ba243d68d4c446e85e19421f1e0269f.png#pic_center" width = 48%>

<center> <img style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src ="https://img-blog.csdnimg.cn/0ba243d68d4c446e85e19421f1e0269f.png#pic_center" width = 48%><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图5 深度为9的树的决策边界（左）与相应的树（右）</div> </center>

想要对新数据点进行预测，首先要查看这个点位于特征空间划分的哪个区域，然后将该区域的多数目标值（如果是纯的叶结点，就是单一目标值）作为预测结果。从根结点开始对树进行遍历就可以找到这一区域，每一步向左还是向右取决于是否满足相应的测试。决策树也可以用于回归任务，使用的方法完全相同。预测的方法是，基于每个结点的测试对树进行遍历，最终找到新数据点所属的叶结点。这一数据点的输出即为此叶结点中所有训练点的平均目标值。

**2. 控制决策树的复杂度**
通常来说，构造决策树直到所有叶结点都是纯的叶结点，这会导致模型非常复杂，并且对训练数据高度过拟合。纯叶结点的存在说明这棵树在训练集上的精度是100%。训练集中的每个数据点都位于分类正确的叶结点中。在图5的左图中可以看出过拟合。你可以看到，在所有属于类别0的点中间有一块属于类别1的区域。另一方面，有一小条属于类别0的区域，包围着最右侧属于类别0的那个点。这并不是人们想象中决策边界的样子，这个决策边界过于关注远离同类别其他点的单个异常点。

防止过拟合有两种常见的策略：一种是及早停止树的生长，也叫预剪枝（pre-pruning）；另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫后剪枝（post-pruning）或剪枝（pruning）。预剪枝的限制条件可能包括限制树的最大深度、限制叶结点的最大数目，或者规定一个结点中数据点的最小数目来防止继续划分。

scikit-learn 的决策树在`DecisionTreeRegressor`类和`DecisionTreeClassifier`类中实现。scikit-learn只实现了预剪枝，没有实现后剪枝。scikit-learn的决策树使用的是CART算法。

scikit-learn使用`sklearn.tree.DecisionTreeClassifier`类来实现决策树分类算法。其中几个典型的参数解释如下：
- criterion：特征选择算法。一种基于信息熵，另一种基于基尼不纯度。有人说两种算法的差异性不大，对模型准确性没有太大影响。
- splitter：创建决策树分支的选项，一种是选择最优的分支创建原则，另外一种是从排名靠前的特征中，随机选择一个特征来创建分支，这个方法和正则项的效果类似，可以避免过拟合。
- max_depth：指定决策树的最大深度，可以避免过拟合。
- min_samples_split：指定能创建分支的数据集的大小，默认是2。这是一种前剪枝的方法。
- min_samples_leaf：创建分支后的节点样本数量必须大于等于这个数值，否则不再创建分支。这是一种前剪枝的方法。
- max_leaf_nodes：除了限制最小的样本节点个数（和参数min_samples_leaf有关），该参数可以限制最大的样本节点个数。
- min_impurity_split：可以使用该参数来指定信息增益的阈值。决策树在创建分支时，信息增益必须大于这个阈值，否则不创建分支。
- 
从这些参数可以看到，scikit-learn有一系列的参数用来控制决策树生成的过程，从而解决过拟合问题。


下面在乳腺癌数据集上更详细地看一下预剪枝的效果。和前面一样，导入数据集并将其分为训练集和测试集。然后利用默认设置来构建模型，默认将树完全展开（树不断分支，直到所有叶结点都是纯的）。我们固定树的random_state，用于在内部解决平局问题：

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split


cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train)))      # Accuracy on training set: 1.000
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))            # Accuracy on test set: 0.937
```

不出所料，训练集上的精度是100%，这是因为叶结点都是纯的，树的深度很大，足以完美地记住训练数据的所有标签。测试集精度比之前讲过的线性模型略低，线性模型的精度约为95%。

如果我们不限制决策树的深度，它的深度和复杂度都可以变得特别大。因此，未剪枝的树容易过拟合，对新数据的泛化性能不佳。现在我们将预剪枝应用在决策树上，这可以在完美拟合训练数据之前阻止树的展开。一种选择是在到达一定深度后停止树的展开。这里我们设置max_depth=4，这意味着只可以连续问4个问题。限制树的深度可以减少过拟合。这会降低训练集的精度，但可以提高测试集的精度：

```python
tree = DecisionTreeClassifier(max_depth=4 ,random_state=0)
tree.fit(X_train, y_train)
print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train)))      # Accuracy on training set: 0.988
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))            # Accuracy on test set: 0.951
```

**3. 分析决策树**

我们可以利用tree 模块的export_graphviz 函数来将树可视化。这个函数会生成一个.dot 格式的文件，这是一种用于保存图形的文本文件格式。我们设置为结点添加颜色的选项，颜色表示每个结点中的多数类别，同时传入类别名称和特征名称，这样可以对树正确标记：

```python
from sklearn.tree import export_graphviz
export_graphviz(tree, out_file="tree.dot", class_names=["malignant", "begin"], feature_names=cancer.feature_names, impurity=False, filled=True)
```

利用`graphviz`模块读取这个文件并将其可视化：

```python
import graphviz

with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

<img src ="https://img-blog.csdnimg.cn/f7431aaee7b44d67ae58887e6f846edf.png#pic_center" width = 48%>

树的可视化有助于深入理解算法是如何进行预测的，也是易于向非专家解释的机器学习算法的优秀示例。不过，即使这里树的深度只有4层，也有点太大了。深度更大的树（深度为10并不罕见）更加难以理解。一种观察树的方法可能有用，就是找出大部分数据的实际路径。上图中每个结点的samples给出了该结点中的样本个数，values 给出的是每个类别的样本个数。观察`worst radius <= 16.795`分支右侧的子结点，我们发现它只包含8个良性样本，但有134个恶性样本。树的这一侧的其余分支只是利用一些更精细的区别将这8个良性样本分离出来。在第一次划分右侧的142个样本中，几乎所有样本（132个）最后都进入最右侧的叶结点中。

再来看一下根结点的左侧子结点，对于`worst radius > 16.795`，我们得到25个恶性样本和259个良性样本。几乎所有良性样本最终都进入左数第二个叶结点中，大部分其他叶结点都只包含很少的样本。


**4. 树的特征重要性**

查看整个树可能非常费劲，除此之外，我还可以利用一些有用的属性来总结树的工作原理。其中最常用的是特征重要性（feature importance），它为每个特征对树的决策的重要性进行排序。对于每个特征来说，它都是一个介于0和1之间的数字，其中0表示“根本没用到”，1表示“完美预测目标值”。特征重要性的求和始终为1：

```python
print("Feature importances:\n{}".format(tree.feature_importances_))
```
> Feature importances:
> [0.         0.         0.         0.         0.         0.
>  0.         0.         0.         0.         0.01019737 0.04839825
>  0.         0.         0.0024156  0.         0.         0.
>  0.         0.         0.72682851 0.0458159  0.         0.
> 0.0141577  0.         0.018188   0.1221132  0.01188548 0.        ]

将特征重要性可视化：

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")
plot_feature_importances_cancer(tree)
```

<img src ="https://img-blog.csdnimg.cn/5bcc67ad64154b8a9b88167b8d84be3d.png#pic_center" width = 48%>

这里我们看到，顶部划分用到的特征（“worst radius”）是最重要的特征。这也证实了我们在分析树时的观察结论，即第一层划分已经将两个类别区分得很好。

但是，如果某个特征的feature_importance_ 很小，并不能说明这个特征没有提供任何信息。这只能说明该特征没有被树选中，可能是因为另一个特征也包含了同样的信息。

与线性模型的系数不同，特征重要性始终为正数，也不能说明该特征对应哪个类别。特征重要性告诉我们“worst radius”（最大半径）特征很重要，但并没有告诉我们半径大表示样本是良性还是恶性。事实上，在特征和类别之间可能没有这样简单的关系，你可以在下面的例子中看出这一点：

```python
tree = mglearn.plots.plot_tree_not_monotone()
display(tree)
```

<img src ="https://img-blog.csdnimg.cn/5a2ce4f7441c41c28489ec9c3366b1c7.png#pic_center" width = 48%>
<img src ="https://img-blog.csdnimg.cn/effde0fc183847e39dc34492c4144f5a.png#pic_center" width = 48%>

该图显示的是有两个特征和两个类别的数据集。这里所有信息都包含在X[1]中，没有用到X[0]。但X[1]和输出类别之间并不是单调关系，即我们不能这么说：“较大的X[1]对应类别0，较小的X[1]对应类别1”（反之亦然）。

**5. 回归的决策树**

虽然这里主要讨论的是用于分类的决策树，但对用于回归的决策树来说，所有内容都是类似的，在`DecisionTreeRegressor`中实现。回归树的用法和分析与分类树非常类似。但在将基于树的模型用于回归时，我们想要指出它的一个特殊性质。`DecisionTreeRegressor`（以及其他所有基于树的回归模型）不能外推（extrapolate），也不能在训练数据范围之外进行预测。


我们利用计算机内存（RAM）历史价格的数据集来更详细地研究这一点。下图给出了这个数据集的图像，x轴为日期，y轴为那一年1兆字节（MB）RAM的价格：

```python
import pandas as pd

ram_prices = pd.read_csv("ram_price.csv")

plt.semilogy(ram_prices.date, ram_prices.price)
plt.xlabel("Year")
plt.ylabel("Price in $/Mbyte")
```
<img src ="https://img-blog.csdnimg.cn/35185697f5a340c58d83741f42b93098.png#pic_center" width = 48%>

注意y轴的对数刻度。在用对数坐标绘图时，二者的线性关系看起来非常好，所以预测应该相对比较容易，除了一些不平滑之处之外。

我们将利用2000年前的历史数据来预测2000年后的价格，只用日期作为特征。我们将对比两个简单的模型：DecisionTreeRegressor 和LinearRegression。我们对价格取对数，使得二者关系的线性相对更好。这对`DecisionTreeRegressor`不会产生什么影响，但对`LinearRegression`的影响却很大。训练模型并做出预测之后，我们应用指数映射来做对数变换的逆运算。为了便于可视化，我们这里对整个数据集进行预测，但如果是为了定量评估，我们将只考虑测试数据集：

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression


# 利用历史数据预测2000年后的价格
data_train = ram_prices[ram_prices.date < 2000]
data_test = ram_prices[ram_prices.date > 2000]

# 基于日期来预测价格
X_train = data_train.date[:, np.newaxis]
# 利用对数变换得到数据和目标之间更简单的关系
y_train = np.log(data_train.price)

tree = DecisionTreeRegressor().fit(X_train, y_train)
linear_reg = LinearRegression().fit(X_train, y_train)

# 对所有数据进行预测
X_all = ram_prices.date[:, np.newaxis]

pred_tree = tree.predict(X_all)
pred_lr = linear_reg.predict(X_all)

# 对数变换逆运算
price_tree = np.exp(pred_tree)
price_lr = np.exp(pred_lr)
```

将决策树和线性回归模型的预测结果与真实值进行对比：

```python
plt.semilogy(data_train.date, data_train.price, label="Training data")
plt.semilogy(data_test.date, data_test.price, label="Test data")
plt.semilogy(ram_prices.date, price_tree, label="Tree prediction")
plt.semilogy(ram_prices.date, price_lr, label="Linear prediction")
plt.legend()
```

<img src ="https://img-blog.csdnimg.cn/12c47a8d245347fb86580e1db2e4033e.png#pic_center" width = 48%>

两个模型之间的差异非常明显。线性模型用一条直线对数据做近似，这是我们所知道的。这条线对测试数据（2000年后的价格）给出了相当好的预测，不过忽略了训练数据和测试数据中一些更细微的变化。与之相反，树模型完美预测了训练数据。由于我们没有限制树的复杂度，因此它记住了整个数据集。但是，一旦输入超出了模型训练数据的范围，模型就只能持续预测最后一个已知数据点。树不能在训练数据的范围之外生成“新的”响应。所有基于树的模型都有这个缺点。


决策树有两个优点：一是得到的模型很容易可视化，非专家也很容易理解（至少对于较小的树而言）；二是算法完全不受数据缩放的影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，比如归一化或标准化。特别是特征的尺度完全不一样时或者二元特征和连续特征同时存在时，决策树的效果很好。

决策树的主要缺点在于，即使做了预剪枝，它也经常会过拟合，泛化性能很差。因此，在
大多数应用中，往往使用后面介绍的集成方法来替代单棵决策树。



## 参考
- 决策树算法原理：[https://www.cnblogs.com/pinard/p/6050306.html](https://www.cnblogs.com/pinard/p/6050306.html)
- 决策树算法：[https://zhuanlan.zhihu.com/p/126294494](https://zhuanlan.zhihu.com/p/126294494)
- 决策树学习算法：[https://blog.csdn.net/sai_simon/article/details/122964676?spm=1001.2014.3001.5502](https://blog.csdn.net/sai_simon/article/details/122964676?spm=1001.2014.3001.5502)
- 决策树及决策树生成与剪枝：[https://blog.csdn.net/am290333566/article/details/81187562](https://blog.csdn.net/am290333566/article/details/81187562)