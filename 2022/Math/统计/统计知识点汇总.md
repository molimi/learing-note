## 1 概率与分布
### 1.1 基本概念
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;统计学研究的一个主要问题是通过试验推断总体。而概率论为整个统计学奠定了基础，它为总体、随机试验等几乎所有随机现象的建模提供了方法。
**（一）样本与事件**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;称某次试验全体可能的结果所构成的集合*S* 为该试验的<font color=#9900CC ><strong> 样本空间</strong></font>（sample space）。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一个<font color=#9900CC><strong>事件</strong></font>（event）是一次试验若干可能的结果所构成的集合，即 *S* 的一个子集（可以是S本身）。

**（二）事件的运算**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设试验 *E* 的样本空间为 *S* ，而 A, B, $A_k$(k= l，2，...）是S的子集。
 1. 若$A \subset B$，则称事件B包含事件A，即事件A发生必导致事件B 发生。若$A \subset B$且$B \subset A$，即A=B，则称事件A与事件B相等。
 2. 事件$A \cup B＝ \{x|x \in A 或 x \in B\}$称为事件A与事件B的和事件。当且仅当A，B中至少有一个发生时，事件$A \cup B$发生。类似地，称 $\bigcup_{i=1}^{n} {A_i}$ 为 *n* 个事件$A_1, A_2, \cdots, A_n$ 的和事件；称 $\bigcup_{i=1}^{\infty} {A_i}$ 为可列个事件$A_1, A_2, \cdots$ 的<font color=#9900CC><strong>和事件</strong></font>。
 3. 事件$A \cap B = \{x|x \in A 且 x \in B\}$称为事件A与事件B的<font color=#9900CC><strong>积事件</strong></font>。当且仅当A，B同时发生时，事件$A \cap B$发生。$A \cap B$也记作AB。类似地，称 $\bigcap_{i=1}^{n} {A_i}$ 为 *n* 个事件$A_1, A_2, \cdots, A_n$ 的积事件；称 $\bigcap_{i=1}^{\infty} {A_i}$ 为可列个事件$A_1，A_2，\cdots$ 的积事件。
 4. 事件$A-B = \{x|x \in A 且 x \notin B\}$ 称为事件A与事件B的<font color=#9900CC><strong>差事件</strong></font>。当且仅当 *A* 发生、*B*不发生时事件 *A-B* 发生。
 5. 若$A \cap B = \emptyset$，则称事件 *A* 与 *B* 是<font color=#9900CC><strong>互不相容的，或互斥的</strong></font>。这指的是事件*A* 与事件 *B* 不能同时发生。基本事件是两两互不相容的。
 6. 若$A \cup B = S且A \cap B = \emptyset$ ， 则称事件 *A* 与事件 *B* 互为<font color=#9900CC><strong>逆事件</strong></font>。又称事件 *A* 与事件 *B* 互为对立事件。这指的是对每次试验而言，事件 *A*、*B* 中必有一个发生，且仅有一个发生。*A* 的对立事件记为$A^C$，有些教材记为 $\bar A$。$A^C=S-A$
 
 补充：
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果事件$A_1, A_2, \cdots$ 两两不交，并且$\bigcap_{i=1}^{\infty} {A_i}=S$，则称$A_1, A_2, \cdots$ 构成 *S* 的一个划分(partition)。

**（三）定理** 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于样本空间 *S* 上的任意事件 *A*, *B* 和 *C*，有:

 1. 交换律：$A \cup B = B \cup A ，A \cap B = B \cap A$
  2. 结合律：$A \cup (B \cup C) = (A \cup B) \cup C，A \cap (B \cap C) = (A \cap B) \cap C$
  3. 分配律：$A \cap (B \cup C) = (A  \cap B)\cup (A \cap C) ，A \cup (B \cap C) = (A\cup B) \cap (A \cup C)$
  4. DeMorgan律：$(A \cup B)^C  =  A^C \cap B^C，  (A \cap B)^C  =  A^C \cup B^C$

**（四）概率函数**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S 的一族子集如果满足下列三个性质，就称作一个 $\sigma$ 代数（sigma algebra）或一个 *Borel* 域（Borel field），记作 $\Beta$。
 1. $\emptyset \in B$（空集属于 $\Beta$）。 
 2. 若 $A \in B$，则$A^C \in B$（$\Beta$ 在补运算下封闭）。
 3. 若 $A_1, A_2, \cdots \in B$，则 $\bigcup_{i=1}^{\infty} {A_i} \in B$
 
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例子：如果样本空间 *S* 有限或者可数，则我们可以直接对 *S* 定义：$B = \{ S 的全体子集，包括 S 本身\}$；若$S = (-\infty，\infty)$为实数轴。令 $\Beta$ 包含全体形如$[a, b], (a, b], (a, b)和[a, b)$的集合，其中 *a*, *b* 为任意实数。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;已知样本空间 *S* 和 $\sigma$ 代数 $\Beta$，定义在$\Beta$上且满足下列条件的函数 *P* 称为一个<font color=#9900CC><strong>概率函数（probability function）</strong></font>：
 1. 对任意 $A \in \Beta$，$P(A) \geq 0$。
 2. $P(S) = 1$。
 3. 若 $A_1，A_2，\cdots \in B$ 且两两不交，则 $P(\bigcup_{i=1}^{\infty} {A_i}) = \sum_{i=1}^{\infty}P(A_i)$。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面三条性质通常称作概率公理（或 Kolmogorov（柯尔莫哥洛夫）公理——以概率论创始人之一 [A. Kolmogorov](https://baike.baidu.com/item/%E5%AE%89%E5%BE%B7%E9%9B%B7%C2%B7%E6%9F%AF%E5%B0%94%E8%8E%AB%E5%93%A5%E6%B4%9B%E5%A4%AB/5356848) 的名字命名，大家会在很多学科遇到这位大牛）。任何满足概率公理的函数 *P* 都称作概率函数。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**有限可加性公理：** 若 $A, B \in \Beta$ 且两者不交，则$P(A \cup B) = P(A) +  P(B)$。

 （五）概率演算
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 设 *P* 是一个概率函数，$A, B \in \Beta$，则
 1. $P(\emptyset) = 0$；
 2. $P(A) \leq 1$；
 3. $P(A^C) = 1 - P(A)$；
 4. $P(B \cap A^C) = P(B) - P(A \cap B)$；
 5. $P(A \cup B) = P(A) +P(B) - P(A \cap B)$；
 6. 若 $A \subset B$，则 $P(A) \leq P(B)$；、
 7. 对任意划分 $C_1, C_2, \cdots$，都有 $P(A) = \sum_{i=1}^{\infty}P(A\cap C_i)$
 8. 对于任意集合 $A_1, A_2, \cdots$，都有 $P(\cup_{i=1}^{\infty}A_i)\leq \sum_{i=1}^{\infty}P(A_i)$

### 1.2 条件概率与独立事件
1. 条件概率：已知  $A$ 事件发生的条件下  $B$ 发生的概率，记作 $P(B|A)$，它等于事件 $AB$ 的概率相对于事件 $A$ 的概率，即：$P(B|A)=\frac{AB}{A}$。其中必须有 $P(A)>0$
2. 条件概率分布的链式法则：对于 $n$ 个随机变量 $X_1,X_2,\cdots,X_n$，有：
$$P(X_1,X_2,\cdots,X_n)=P(X_1)\prod_{i=2}^{n}P(X_i \mid X_1,\cdots,X_{i-1})\tag{1-1}$$

3. 两个随机变量 $X,Y$ 相互独立的数学描述：$P(X,Y)=P(X)P(Y)$。记作：$X \bot Y$。

4. 两个随机变量 $X,Y$关于随机变量 $Z$ 条件独立的数学描述：$P(X,Y\mid Z)=P(X\mid Z)P(Y\mid Z)$。记作：$X\bot Y|Z$。

### 1.3 联合概率分布
1. 定义 $X$ 和 $Y$ 的联合分布为：$P(a,b)=P\{X \le a, Y \le b\}, \quad - \infty \lt a,b \lt + \infty$
 	- $X$ 的分布可以从联合分布中得到：
$$P_X(a)=P\{X \le a\}=P\{X \le a, Y \le \infty\}=P(a,\infty),\quad  - \infty \lt a \lt + \infty\tag{1-2}$$
	 - $Y$ 的分布可以从联合分布中得到：
$$P_Y(b)=P\{Y \le b\}=P\{X \le \infty, Y \le b\}=P(\infty,b), \quad - \infty \lt b \lt + \infty\tag{1-3}$$
2. 当 $X$ 和 $Y$ 都是离散随机变量时，定义 $X$ 和 $Y$ 的联合概率质量函数为：
 $$p(x,y)=P\{X=x,Y=y\}\tag{1-4}$$
则 $X$ 和 $Y$ 的概率质量函数分布为：
$$p_X(x)=\sum_{y}p(x,y) \quad  p_Y(y)=\sum_{x}p(x,y)\tag{1-5}$$
3. 当  $X$ 和 $Y$ 联合地连续时，即存在函数 $p(x,y)$，使得对于所有的实数集合 $\mathbb{A}$ 和 $\mathbb{B}$ 满足：
$$P\{X \in \mathbb A, Y \in \mathbb B\}=\int_\mathbb B \int_\mathbb A p(x,y) dx dy\tag{1-6}$$
则函数 $p(x,y)$ 称为 $X$ 和 $Y$ 的概率密度函数。

	- 联合分布为：$P(a,b)=P\{X \le a, Y \le b\}=  \int_{-\infty}^{a} \int_{-\infty}^{b} p(x,y) dx dy$。

	- $X$ 和 $Y$ 的分布函数以及概率密度函数分别为：
 $$P_X(a)=\int_{-\infty}^{a} \int_{-\infty}^{\infty} p(x,y) dx dy =\int_{-\infty}^{a} p_X(x)dx\\
P_Y(b)=\int_{-\infty}^{\infty} \int_{-\infty}^{b} p(x,y) dx dy=\int_{-\infty}^{b} p_Y(y)dy\\
p_X(x)=\int_{-\infty}^{\infty} p(x,y) dy\\
p_Y(y)=\int_{-\infty}^{\infty} p(x,y) dx\tag{1-7}$$

___


## 2 期望和方差
### 2.1 期望
1. 期望描述了随机变量的平均情况，衡量了随机变量 $X$ 的均值。它是概率分布的泛函（函数的函数）。
	- 离散型随机变量 $X$ 的期望：$\mathbb E[X]=\sum_{i=1}^{\infty}x_ip_i$。若右侧级数不收敛，则期望不存在。
	- 连续性随机变量 $X$ 的期望：$\mathbb E[X]=\int_{-\infty}^{\infty}xp(x)dx$。 若右侧极限不收敛，则期望不存在。

2. 定理：对于随机变量 $X$，设 $Y=g(x)$ 也为随机变量，$g(\cdot)$ 是连续函数。
	- 若 $X$ 为离散型随机变量，且 $Y$ 的期望存在，则：$\mathbb E[Y]=\mathbb E[g(X)]=\sum_{i=1}^{\infty}g(x_i)p_i$。也记做：$\mathbb E_{X\sim P(X)}[g(X)]=\sum_{x}g(x)p(x)$。
	- 若 $X$ 为连续型随机变量，且 $Y$ 的期望存在，则：$\mathbb E[Y]=\mathbb E[g(X)]=\int_{-\infty}^{\infty}g(x)p(x)dx$。也记做：$\mathbb E_{X\sim P(X)}[g(X)]=\int g(x)p(x)dx$。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该定理的意义在于：==当求 $\mathbb E_{Y}$ 时，不必计算出 $Y$ 的分布，只需要利用 $X$ 的分布即可。==

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该定理可以推广至两个或两个以上随机变量的情况。对于随机变量 $X,Y$，假设 $Z=g(X,Y)$ 也是随机变量，$g(\cdot)$ 为连续函数，则有：$\mathbb E[Z]=\mathbb E[g(X,Y)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)p(x,y)dxdy$。也记做：$\mathbb E_{X,Y\sim P(X,Y)}[g(X,Y)\int g(x,y)p(x,y)dxdy$。

3. 期望性质：
	- 常数的期望就是常数本身。
	- 对常数 $C$ 有：$\mathbb E[CX]=C\mathbb E[X]$。
	- 对两个随机变量 $X,Y$，有：$\mathbb E[X+Y]=\mathbb E[X]+\mathbb E[Y]$。该结论可以推广到任意有限个随机变量之和的情况。
	- 对两个相互独立的随机变量 $X,Y$，有：$\mathbb E[XY]=\mathbb E[X]\mathbb E[Y]$。该结论可以推广到任意有限个相互独立的随机变量之积的情况。

### 2.2 方差
1. 对随机变量 $X$，若 $\mathbb E[(X-\mathbb E[X])^{2}]$ 存在，则称它为 $X$ 的方差，记作 $Var[X]$。
 $X$ 的标准差为方差的开平方。即：
 $$Var[X]=\mathbb E[(X-\mathbb E[X])^{2}] \\
\sigma=\sqrt{Var[X]}\tag{2-1}$$

	- 方差度量了随机变量 $X$ 与期望值偏离的程度，衡量了 $X$ 取值分散程度的一个尺度。
	- 由于绝对值 $|X-\mathbb E[X]|$ 带有绝对值，不方便运算，因此采用平方来计算。又因为 $|X-\mathbb E[X]|^2$ 是一个随机变量，因此对它取期望，即得 $X$ 与期望值偏离的均值
2. 根据定义可知：
$$Var[X]=\mathbb E[(X-\mathbb E[X])^{2}]=\mathbb E[X^{2}]-(\mathbb E[X])^{2}\\
Var [f(X)]=\mathbb E[(f(X)-\mathbb E[f(X)])^{2}]\tag{2-2}$$
3. 对于一个期望为 $\mu$， 方差为 $\sigma^2, \sigma\not=0$ 的随机变量 $X$，随机变量 $X^{*}=\frac {X-\mu}{\sigma}$ 的数学期望为0，方差为1。 称 $X^{*}$ 为 $X$ 的标准化变量。
4. 方差的性质：
	- 常数的方差恒为 0 。
	- 对常数 $C$，有 $Var[CX]=C^2Var[X]$。
	- 对两个随机变量 $X, Y$，有： $Var[X+Y]=Var[X] +Var[Y] +2\mathbb E[(X-\mathbb E[X])(Y-\mathbb E[Y])]$；当 $X$ 和 $Y$ 相互独立时，有 $Var[X+Y] = Var[X] +Var[Y]$。这可以推广至任意有限多个相互独立的随机变量之和的情况。
	- $Var[X]=0$ 的充要条件是 $X$ 以概率1取常数。

### 2.3 协方差和相关系数
1. 对于二维随机变量 $(X, Y)$，可以讨论描述 $X$ 与 $Y$ 之间相互关系的数字特征。
	- 定义：$\mathbb E[(X-\mathbb E[X])(Y-\mathbb E [Y])]$ 为随机变量 $X$ 与 $Y$ 的协方差，记作 $Cov[ X,Y]=\mathbb E[(X-\mathbb E[X])(Y-\mathbb E [Y])]$。
	- 定义：$\rho_{XY}=\frac {Cov[X,Y]}{\sqrt{Var[X] }\sqrt{Var[Y]}}$ 为随机变量 $X$ 与 $Y$ 的相关系数，它是协方差的归一化。
2. 由定义可知：
$$Cov[ X,Y] =Cov[ Y,X] \\
Cov [X,X] =Var [X] \\
Var [X+Y] =Var [X] +Var [Y] +2Cov [X,Y] \tag{2-3}$$
3. 协方差的性质：
	- $Cov [aX,bY] =abCov [X,Y], a,b为常数$
	- $Cov[ X_1+X_2,Y ]=Cov [X_1,Y] +Cov [X_2,Y]$
	- $Cov [f(X),g(Y)]=\mathbb E[(f(X)-\mathbb E[f(X)])(g(Y)-\mathbb E[g(Y)])]$
	- $\rho[f(X),g(Y)]=\frac {Cov[f(X),g(Y)]}{\sqrt{Var[f(X)] }\sqrt{Var[g(Y)]}}$
4. 协方差的物理意义：
	- 协方差的绝对值越大，说明两个随机变量都远离它们的均值。
	- 协方差如果为正，则说明两个随机变量同时趋向于取较大的值或者同时趋向于取较小的值；如果为负，则说明一个随变量趋向于取较大的值，另一个随机变量趋向于取较小的值。
	- <font color=#9900CC><strong>两个随机变量的独立性可以导出协方差为零。但是两个随机变量的协方差为零无法导出独立性。因为独立性也包括：没有非线性关系。有可能两个随机变量是非独立的，但是协方差为零。</font></strong>如：假设随机变量 $X\sim U[-1,1]$。定义随机变量 $S$ 的概率分布函数为：
$$P(S=1)= \frac 12P(S=-1)= \frac 12$$
定义随机变量 $Y=SX$，则随机变量 $X,Y$ 是非独立的，但是有：$Cov[X,Y]=0$。
5. 相关系数的物理意义：考虑以随机变量 $X$ 的线性函数 $a+bX$ 来近似表示 $Y$。以均方误差
$$e=\mathbb E[(Y-(a+bX))^{2}]=\mathbb E[Y^{2}] +b^{2}\mathbb E[X^{2}] +a^{2}-2b\mathbb E[XY] +2ab\mathbb E[X] -2a\mathbb E [Y]\tag{2-4}$$
来衡量以 $a+bX$ 近似表达 $Y$ 的好坏程度。$e$ 越小表示近似程度越高。
为求得最好的近似，则对 $a,b$ 分别取偏导数，得到：
$$a_0=\mathbb E[Y] -b_0\mathbb E[X] =\mathbb E[Y] -\mathbb E[X] \frac{Cov [X,Y]}{Var [X] }\\
b_0=\frac{Cov[ X,Y] }{Var[ X] }\\
\min(e)=\mathbb E[(Y-(a_0+b_0X))^{2}]=(1-\rho^{2}_{XY})Var [Y] \tag{2-5}$$
因此有以下定理：
	- $|\rho_{XY}| \le 1$（ $|\cdot|$ 是绝对值）。
	- $|\rho_{XY}=1|$ 的充要条件是：存在常数 $a, b$ 使得 $P\{Y=a+bX\}=1$。
6. 当 $|\rho_{XY}|$ 较大时，$e$ 较小，意味着随机变量 $X$ 和 $Y$ 联系较紧密。于是 $|\rho_{XY}|$ 是一个表征 $X、Y$ 之间线性关系紧密程度的量。

7. 当 $|\rho_{XY}|=0$ 时，称 $X$ 和 $Y$ 不相关。
	- 不相关是就线性关系来讲的，而相互独立是一般关系而言的。
	- 相互独立一定不相关；不相关则未必独立。

### 2.4 协方差矩阵
1. 设 $X$ 和 $Y$ 是随机变量
	- 若  $\mathbb{E}[X^k], k=1, 2,\cdots$ 存在，则称它为 $X$ 的 $k$ 阶原点矩，简称 $k$ 阶矩。
	- 若 $\mathbb E[(X-\mathbb E[X])^{k}] ,k=2,3,\cdots$ 存在，则称它为 $X$ 的 $k$ 阶中心矩。
	- 若 $\mathbb E[X^{k}Y^{l}] ,k,l=1,2,\cdots$ 存在，则称它为 $X$ 和 $Y$ 的 $k+l$ 阶混合矩。
	- 若 $\mathbb E[(X-\mathbb E[X])^{k}(Y-\mathbb E[Y])^{l}] ,k,l=1,2,\cdots$ 存在，则称它为 $X$ 和 $X$ 的 $k+l$ 阶混合中心矩。
因此：期望是一阶原点矩，方差是二阶中心矩，协方差是二阶混合中心矩。

2. 协方差矩阵：
	- 二维随机变量 $(X_1,X_2)$ 有四个二阶中心矩（假设他们都存在），记作：
$$c_{11}=\mathbb E[(X_1-\mathbb E[X_1])^{2}] \\
c_{12}=\mathbb E[(X_1-\mathbb E[X_1])( X_2-\mathbb E[X_2]) ]  \\
c_{21}=\mathbb E[( X_2-\mathbb E[X_2])(X_1-\mathbb E[X_1] ) ]  \\
c_{22}=\mathbb E[(X_2-\mathbb E[X_2])^{2}] \tag{2-6}$$
称矩阵
$$\mathbf C=\begin{bmatrix}
c_{11}&c_{12}\\
c_{21}&c_{22}
\end{bmatrix}\tag{2-7}$$
为随机变量 $(X_1,X_2)$ 的协方差矩阵。
	- 设 $n$ 维随机变量 $(X_1, X_2,\cdots,X_n)$ 的二阶混合中心矩 $c_{ij}=Cov [X_i,X_j] =\mathbb E[(X_i-\mathbb E[X_i] )( X_j-\mathbb E[X_j] ) ]$ 都存在，则称矩阵
$$\mathbf C=
\begin{bmatrix}
c_{11} & c_{12} & \cdots & c_{1n} \\
c_{21} & c_{22} & \cdots & c_{2n} \\
\vdots &\vdots &\ddots &\vdots \\
c_{n1} & c_{n2} & \cdots & c_{nn} \\
\end{bmatrix}\tag{2-8}$$
为 $n$ 维随机变量 $(X_1, X_2,\cdots,X_n)$ 的协方差矩阵。
由于 $c_{ij}=c_{ji}, i\ne j, i,j=1,2,\cdots,n$，因此协方差矩阵是个对称阵。
3. 通常 $n$ 维随机变量的分布是不知道的，或者太复杂以致数学上不容易处理。因此实际中协方差矩阵非常重要。

___


## 3 大数定律及中心极限定理
### 3.1 切比雪夫不等式
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设随机变量 $X$ 具有期望 $E(X) = \mu$，方差 $D(X) = \sigma^2$，则对于任意正数 $\epsilon$，下面的不等式成立：
$$P\{ \lvert X - \mu \rvert \geq \epsilon \} \leq \frac{\epsilon^2}{\sigma^2}\tag{3-1}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其意义是：对于距离 $E(X)$ 足够远的地方（距离大于等于 $\epsilon$），事件出现的概率是小于等于 $\frac{\sigma^2}{\epsilon^2}$ 。<font color=#9900CC><strong>即事件出现在区间 $[\mu - \epsilon, \mu + \epsilon]$ 的概率大于 $1 - \frac{\sigma^2}{\epsilon^2}$ 。</strong></font> 所以该不等式给出了随机变量 $X$  在分布未知的情况下，事件 $\{ \lvert  X -\mu \rvert\ \leq \epsilon\}$ 的下限估计。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;证明：
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们以连续随机变量的情况来证明，离散的情况类似可证。设连续随机变量 $X$ 的密度函数为 $f(x)$，事件 $X$ 即表示 $X$ 落在区间 $(\mu - \epsilon, \mu + \epsilon)$ 外，因此在积分范围内恒有 $\frac{(x - \mu)^2}{\epsilon^2} \geq 1$，故：

$$P \{ \lvert X - \mu \rvert \geq \epsilon \}= \int_{\lvert X - \mu \rvert \geq \epsilon }p(x)dx \leq \int_{\lvert X - \mu \rvert \geq \epsilon }\frac{{\lvert X - \mu \rvert}^2}{\epsilon^2}p(x)dx \\
\leq \frac{1}{\epsilon^2} \int_{-\infty}^{\infty} {\lvert X - \mu \rvert}^2p(x)dx = \frac{\sigma^2}{\epsilon^2}\tag{3-2}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;切比雪夫不等式的特殊情况：设随机变量 $X_1, X_2,\cdots,X_n,\cdots$ 相互独立，且具有相同的数学期望和方差：$\mathbb E[X_k] =\mu, Var[X_k] =\sigma^{2}$。 作前 $n$ 个随机变量的算术平均：$\overline X =\frac {1}{n} \sum _{k=1}^{n}X_k$， 则对于任意正数 $\varepsilon$ 有：
$$\lim_{n\rightarrow \infty}P\{|\overline X-\mu| \lt \varepsilon\}=\lim_{n\rightarrow \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}X_k-\mu| \lt \varepsilon\} =1\tag{3-3}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;证明：根据期望和方差的性质有：$\mathbb E[\overline X]=\mu$，$Var[\overline X]=\frac{\sigma^2}{n}$。根据切比雪夫不等式有：
$$P\{|\overline X-\mu| \ge \varepsilon\} \le \frac{\sigma^2}{n\varepsilon^2}\tag{3-4}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则有 $\lim_{n\rightarrow \infty}P\{|\overline X-\mu| \ge \varepsilon\} = 0$，因此有：$\lim_{n\rightarrow \infty}P\{|\overline X-\mu| \lt \varepsilon\} =1$
### 3.2 大数定律
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[大数定律](https://wiki.mbalib.com/wiki/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B)（Law of Large Numbers，LLN）是指某个随机事件在单次试验中可能发生也可能不发生，但在大量重复实验中往往呈现出明显的规律性，即该随机事件发生的频率会向某个常数值收敛，该常数值即为该事件发生的概率。另一种表达方式为<font color=#9900CC><strong>当样本数据无限大时，样本均值趋于总体均值。</strong></font>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大数定律的作用：现实生活中，我们无法进行无穷多次试验，也很难估计出总体的参数。大数定律告诉我们能用频率近似代替概率；能用样本均值近似代替总体均值。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大数定律的表达方式主要有：辛钦大数定律、切比雪夫（Cheby—shev）大数法则、贝努利（Bernoulli）大数法则。
#### 3.2.1 辛钦大数定律
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;弱大数定律即辛钦大数定律，设 $X_1, X_2, \cdots$ 是相互独立，服从同一分布的随机变量序列，且具有数学期望 $E(X_k) = \mu (k = 1, 2, \cdots)$。前 $n$ 个变量的算术平均 $\frac{1}{n} \sum_{k = 1}^{n}X_k$，则对任意 $\epsilon > 0$，有
$$ \lim_{n \to \infty}P\{\lvert \frac{1}{n} \sum_{k = 1}^{n}X_k - \mu \rvert < \epsilon \}  = 1\tag{3-5}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;辛钦大数定理从理论上指出，<font color=#9900CC><strong>对于独立同分布且具有均值 $\mu$ 的随机变量 $X_1, \cdots, X_n$，当 $n$ 很大时，用它们的算术平均值 $\frac{1}{n} \sum_{k = 1}^{n}X_k$ 来近似实际真值 $\mu$ 是合理的。</strong></font>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外一种描述，设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，服从同一分布且具有数学期望 $E(X_k) = \mu (k = 1, 2, \cdots)$，则序列 $\overline{X} = \frac{1}{n} \sum_{k = 1}^{n}X_k$ 依概率收敛于 $\mu$，即 $\overline{X} \overset{P}{\to}  \mu$。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注：（1）这里并没有要求随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 的方差存在。（2）当 $X$ 为服从0-1分布的随机变量时，辛钦大数定律就是伯努利大数定律，故伯努利大数定律是辛钦伯努利大数定律的一个特例。

#### 3.2.2 伯努利大数定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设 $f_A$ 是 $n$ 次独立重复试验中事件 $A$ 发生的次数， $p$ 是事件 $A$ 在每次试验中发生的概率，则对于任意正数 $\epsilon > 0$，有：
$$ \lim_{n \to \infty}P\{\lvert \frac{f_A}{n} - p \rvert < \epsilon \}  = 1 \quad 或 \quad \lim_{n \to \infty}P\{\lvert \frac{f_A}{n} - p \rvert  \geq \epsilon \}  = 0\tag{3-6}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伯努利大数定律说明：<font color=#9900CC><strong>当独立重复实验执行非常大的次数时，事件 $A$ 发生的频率逼近于它的概率。</strong></font>
#### 3.2.3 切比雪夫大数定律
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设随机变量 $X_1, X_2, \cdots, X_n$ 相互独立，分别有数学期望 $E(X_1), E(X_2), \cdots, E(X_n)$ 及方差 $D(X_1), D(X_2), \cdots, D(X_n)$ 并且方差是一致有界的，即存在某一个常数 $K$，使得 $D(X_k)< K, k=1, 2, \cdots$ 则对任意 $\epsilon > 0$，恒有 
$$\lim_{n \to \infty}P(\lvert \frac{1}{n} \sum_{k=1}^{n}X_k - \frac{1}{n} \sum_{i=1}^{n}E(X_k) \rvert < \epsilon) =1\tag{3-7}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;切比雪夫大数定理的意义：由于独立随机变量 $X_1, X_2, \cdots, X_n$ 的算术平均值 $\overline{X_n} = \frac{1}{n} \sum_{k=1}^{n} X_k$ 的数学期望 $E(\overline{X_n}) = \frac{1}{n} \sum_{k=1}^{n} E(X_k)$ 及方差 $D(\overline{X_n}) = \frac{1}{n^2} \sum_{k=1}^{n} D(X_k)$，当各个方差一致有界时，$D(\overline{X_n}) < \frac{1}{n^2}nK = \frac{K}{n}$，由此可见，<font color=#9900CC><strong>当 $n$ 充分大时，随机变量 $X_n$ 的分布的分散度是很小的，$\overline{X_n}$ 的值比较集中在其数学期望附近。</strong></font>


#### 3.2.4 马尔科夫大数定律

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;马尔科夫条件：
$$\lim_{n \to \infty} \frac{D(\sum_{k =1}^{n}X_k)}{n^2} = 0\tag{3-8}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;满足马尔科夫条件的随机变量序列 $X_n$ 服从大数定律。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**小结：**
|大数定理| 分布 |期望	|	方差|	用途|
|--|--|--|--|--|
|伯努利  | 二项分布 |	相同|	相同|估算概率	|
| 辛钦 | 独立同分布 |	相同|相同	|估算期望	|
| 切比雪夫 |独立  |	存在|存在  有限	|估算期望	|

> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[小概率原理](https://baike.baidu.com/item/%E5%B0%8F%E6%A6%82%E7%8E%87%E5%8E%9F%E7%90%86/10876969)或实际推断原理——一个事件如果发生的概率很小的话，那么它在一次试验中是几乎不可能发生的，但在多次重复试验中几乎是必然发生的。统计学里，把小概率事件在一次实验中看成是实际不可能发生的事件，一般认为等于或小于0.05或0.01的概率为小概率。实际推断原理通常在假设检验中使用，即如果小概率事件在一次试验中居然发生了，则有理由首先怀疑原假设的真实性，从而拒绝原假设。
### 3.3 中心极限定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;==当样本量 $n$ 逐渐趋于无穷大时，$n$ 个抽样样本的均值的频数逐渐趋于正态分布，其对原总体的分布不做任何要求，意味着无论总体是什么分布，其抽样样本的均值的频数的分布都随着抽样数的增多而趋于正态分布。== 如下图所示：

<img src="https://img-blog.csdnimg.cn/901ba06d70d348b09a17d56bd3a0584d.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6ZW_6Lev5ryr5ryrMjAyMQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width=36%>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注：图中虚线表示正态分布，可以发现当样本量 $n$ 逐渐增大时，样本的分布趋于正态分布。

#### 3.3.1 独立同分布的中心极限定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，服从同一分布，且具有数学期望和方差： $E(X_k) = \mu, \quad D(X_k) = \sigma^2 (k = 1, 2, \cdots)$，则随机变量之和 $\sum_{k = 1}^{n}X_k$ 的标准化变量
$$ Y_n = \frac{\sum_{k=1}^{n}X_k - E(\sum_{k=1}^{n}X_k)}{\sqrt{D(\sum_{k=1}^{n}X_k)}} = \frac{\sum_{k=1}^{n}X_k - n\mu}{\sqrt n \sigma}\tag{3-9}$$
的分布函数 $F_n(x)$ 对于任意 $x$ 满足
$$   \lim_{n \to \infty}F_n(x) = \lim_{n \to \infty}P\{\frac{ \sum_{k = 1}^{n}X_k - n \mu}{\sqrt{n} \sigma} \leq x \}  = \int_{-\infty}^{x}\frac{1}{\sqrt{2 \pi}}e^{-t^2/2}dt = \Phi(x)\tag{3-10}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其物理意义为：<font color=#9900CC><strong>均值方差为  $\mu, \sigma^2$ 的独立同分布的随机变量  $X_1, X_2, \cdots, X_n$ 之和 $\sum_{k = 1}^{n}X_k$ 的标准变化量 $Y_n$，当 $n$ 充分大时，其分布近似于标准正态分布。</strong></font> 一般情况下，很难求出 $n$ 个随机变量之和的分布函数。因此当 $n$ 充分大时，可以通过正态分布来做理论上的分析或者计算。

####  3.3.2 棣莫弗-拉普拉斯定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;棣莫弗-拉普拉斯（De Moivre-Laplace）中心极限定理是独立同分布中心极限定理的特殊情况，它是最先被发现的中心极限定理。设随机变量 $\eta_n(n=1, 2, \cdots)$ 服从参数为 *n*, *p*(0<p<1)的二项分布，则对于任意 *a*，有：
$$  \lim_{n \to \infty}P\{\frac{\eta_n - np}{\sqrt{np(1-p)}} \leq a \}  = \int_{-\infty}^{x}\frac{1}{\sqrt{2 \pi}}e^{-t^2/2}dt = \Phi(x)\tag{3-11}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该定理表明，<font color=#9900CC><strong>正态分布是二项分布的极限分布。当 $n$ 充分大时，我们可以利用上式来计算二项分布的概率。</strong></font>

#### 3.3.3 独立不同分布下的中心极限定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;长度、重量、时间等等实际测量量一般符合正态分布，因为它们受各种微小的随机因素的扰动。这些随机因素的独立性是很普遍的，但很难说它们一定同分布。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上，一系列独立不同分布的随机变量也可能满足中心极限定理，只是这些不同分布的随机变量要有所限制。以下给出两个独立不同分布下的中心极限定理，不予证明，简单扩展一下。
#### 3.3.4 林德伯格中心极限定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设 $\{ X_n\}$ 是一系列相互独立的连续随机变量，它们具有有限的期望 $E(X_k) = \mu_k$ 和方差 $D(X_k) = \sigma_{k}^{2}$ ，记 $Y_n = \sum_{k = 1}^{n}X_k, \quad D(Y_n) = \sum_{k = 1}^{n}\sigma_{k}^{2} = B_{n}^{2}$ ，记 $X_k$ 的概率密度函数是 $f_i(x)$，若
$$\forall \tau > 0：\lim_{n \to \infty} \frac{1}{\tau^2 B_n^2}\sum_{k=1}^{n} \lvert X - \mu \rvert \geq \tau B_n (x - \mu)^2f_k(x)dx = 0\tag{3-12}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则
$$\lim_{n \to \infty}P(\frac{1}{B_n}\sum_{k = 1}^{n}(X_k - \mu) < a) = \Phi(a)\tag{3-13}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;林德伯格中心极限定理对 $\{ X_n \}$ 的约束基本上是最弱的，也就是最强的中心极限定理。

#### 3.3.5 Lyapunov定律
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，他们具有数学期望和方差：
$$E(X_k) = \mu_k，\quad D(X_K) = \sigma_{k}^2>0，k = 1, 2, \cdots\tag{3-14}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;记                                                  $B_{n}^2 = \sum_{k = 1}^{n} \sigma_{k}^2$ 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;若存在正数 $\delta$，使得当 $n \to \infty$ 时，
$$\frac{1}{B_{n}^{2 + \delta}} \sum_{k=1}^{n}E\{{\lvert X_k - \mu_k \rvert }^{2 + \delta}\} \to 0\tag{3-15}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则随机变量之和 $\sum_{k = 1}^{n}X_k$ 的标准化变量
$$ Z_n = \frac{\sum_{k = 1}^{n}X_k - E(\sum_{k = 1}^{n}X_k)}{\sqrt{D(\sum_{k = 1}^{n}X_k)}} = \frac{\sum_{k = 1}^{n}X_k - \sum_{k = 1}^{n}\mu_k}{B_n}\tag{3-16}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;的分布函数 $F_n(x)$ 对于任意 $x$，满足
$$\lim_{n \to \infty}F_n(x) = \lim_{n \to \infty}P\{ \frac{\sum_{k = 1}^{n}X_k - \sum_{k = 1}^{n}\mu_k}{B_n}  \leq x\} \\
=  \int_{-\infty}^{x}\frac{1}{\sqrt{2 \pi}}e^{-t^2/2}dt = \Phi(x)\tag{3-17}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该定理表明，随机变量
$$Z_n =  \frac{\sum_{k = 1}^{n}X_k - \sum_{k = 1}^{n}\mu_k}{B_n}\tag{3-18}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当 $n$ 很大时，近似地服从正态分布 $N(0，1)$。 由此，当 $n$ 很大时，$\sum_{k = 1}^{n}X_k  = B_nZ_n + \sum_{k = 1}^{n}\mu_k$ 近似地服从正态分布 $N(\sum_{k = 1}^{n}\mu_k, B_n^2 )$。这就是说，<font color=#9900CC><strong>无论各个随机变量 $X_k(k=1, 2, \cdots)$ 服从什么分布，只要满足定理的条件，那么它们的 $\sum_{k = 1}^{n}X_k$，当 $n$ 很大时，就近似地服从正态分布。</strong></font>

____


## 4 高斯分布
### 4.1 一维正态分布
1. 正态分布的概率密度函数为 :
$$p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^{2}/ (2\sigma^{2})}, -\infty \lt x \lt \infty\tag{4-1}$$
其中 $\mu,\sigma(\sigma>0)$ 为常数。
- 若随机变量 $X$ 的概率密度函数如上所述，则称 $X$ 服从参数为 $\mu,\sigma$ 的正态分布或者高斯分布，记作 $X\sim N(\mu,\sigma^2)$。
- 特别的，当 $\mu=0,\sigma=1$ 时，称为标准正态分布，其概率密度函数记作 $\varphi(x)$，分布函数记作 $\Phi(x)$。
- 为了计算方便，有时也记作：$\mathcal N(x;\mu,\beta^{-1}) =\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{1}{2}\beta(x-\mu)^{2}\right)$，其中 $\beta \in (0,\infty)$。
2. 正态分布的概率密度函数性质：
- 曲线关于 $x=\mu$ 对称。
- 曲线在 $x=\mu$ 时取最大值。
- 曲线在 $x=\mu \pm \sigma$ 处有拐点。
- 参数 $\mu$ 决定曲线的位置； $\sigma$ 决定图形的胖瘦。
<img src="https://img-blog.csdnimg.cn/af6e8612949f4c8ea34b2ff590bf7fc0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZW_6Lev5ryr5ryrMjAyMQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width=50%>

3. 若 $X\sim N(\mu,\sigma^2)$ 则：
- $\frac{X-\mu}{\sigma} \sim N(0,1)$
- 期望：$\mathbb E[X] = \mu$。方差：$Var[X]=\sigma^2$。
4. 有限个相互独立的正态随机变量的线性组合仍然服从正态分布：若随机变量 $X_i \sim N(\mu_i,\sigma_i^{2}),i=1,2,\cdots,n$ 且它们相互独立，则它们的线性组合：$C_1X_1+C_2X_2+\cdots+C_nX_n$，仍然服从正态分布（其中 $C_1,C_2,\cdots C_n$ 不全是为 0 的常数），且：$C_1X_1+C_2X_2+\cdots+C_nX_n \sim N(\sum_{i=1}^{n}C_i\mu_i,\sum_{i=1}^{n}C_i^{2}\sigma_i^{2})$。

### 4.2 多维正态分布
1. 二维正态随机变量 $(X,Y)$ 的概率密度为：
$$p(x,y)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^{2}}}\exp\{\frac{-1}{2(1-\rho^{2})}[\frac{(x-\mu_1)^{2}}{\sigma_1^{2}}\\
-2\rho\frac{(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}+\frac{(y-\mu_2)^{2}}{\sigma_2^{2}}]\}\tag{4-2}$$
根据定义，可以计算出:
$$p_X(x)=\frac{1}{\sqrt{2\pi}\sigma_1}e^{-(x-\mu_1)^{2}/ (2\sigma_1^{2})}, -\infty \lt x \lt \infty \\
p_Y(y)=\frac{1}{\sqrt{2\pi}\sigma_2}e^{-(y-\mu_2)^{2}/ (2\sigma_2^{2})}, -\infty \lt y \lt \infty\\
\mathbb E[X] =\mu_1 \\
\mathbb E[Y] =\mu_2 \\
Var[X] =\sigma_1^{2} \\
Var[Y]=\sigma_2^{2}\\
Cov[X,Y]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x-\mu_1)(y-\mu_2)p(x,y)dxdy=\rho \sigma_1\sigma_2\\
\rho_{XY}=\rho\tag{4-3}$$

2. 引入矩阵：
$$\mathbf{\vec x}=\begin{bmatrix}
x \\
y
\end{bmatrix} \quad
\mathbf{\vec \mu}=\begin{bmatrix}
\mu_1 \\ 
\mu_2
\end{bmatrix}\quad 
\mathbf{\Sigma}=\begin{bmatrix}
c_{11}  &c_{12}\\
c_{21}  &c_{22}
\end{bmatrix} =
\begin{bmatrix}
\sigma_1^{2} & \rho \sigma_1 \sigma_2 \\ 
\rho \sigma_1 \sigma_2 & \sigma_2^{2}
\end{bmatrix}\tag{4-4}$$

 $\mathbf{\Sigma}$ 为 $(X,Y)$ 的协方差矩阵。其行列式为 $\det \mathbf{\Sigma} =\sigma_1^{2}\sigma_2^{2}(1-\rho^{2})$，其逆矩阵为：
$$\mathbf{\Sigma}^{-1}=\frac{1}{\det\mathbf \Sigma}\begin{bmatrix}
\sigma_2^{2} & -\rho \sigma_1 \sigma_2 \\ 
-\rho \sigma_1 \sigma_2 & \sigma_1^{2}
\end{bmatrix}\tag{4-5}$$
于是 $(X,Y)$ 的概率密度函数可以写作 $( \mathbf {\vec x}- \mathbf {\vec \mu})^{T}$ 表示矩阵的转置：
$$p(x,x)=\frac{1}{(2\pi)(\det \mathbf \Sigma)^{1/ 2}}\exp\{- \frac 12 ( \mathbf {\vec x}- \mathbf {\vec \mu})^{T} \mathbf \Sigma^{-1}( \mathbf {\vec x}- \mathbf {\vec \mu})\}\tag{4-6}$$
其中：
- 均值 $\mu_1,\mu_2$ 决定了曲面的位置（本例中均值都为0）。
- 标准差 $\sigma_1,\sigma_2$ 决定了曲面的陡峭程度（本例中方差都为1）。
-  $\rho$ 决定了协方差矩阵的形状，从而决定了曲面的形状。
	- $\rho=0$ 时，协方差矩阵对角线非零，其他位置均为零。此时表示随机变量之间不相关。
此时的联合分布概率函数形状如下图所示，曲面在 $z=0$ 平面的截面是个圆形：
<img src="https://img-blog.csdnimg.cn/ce72db846ba142d38a21fcdf36d91ca9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZW_6Lev5ryr5ryrMjAyMQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width=36%>

	- $\rho=0.5$ 时，协方差矩阵对角线非零，其他位置非零。此时表示随机变量之间相关。
此时的联合分布概率函数形状如下图所示，曲面在 $z=0$ 平面的截面是个椭圆，相当于圆形沿着直线 $y=x$ 方向压缩 ：
<img src="https://img-blog.csdnimg.cn/d91a6259ff2349c78da70611750a6390.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZW_6Lev5ryr5ryrMjAyMQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width=36%>

 	 - $\rho=1$ 时，协方差矩阵对角线非零，其他位置非零。
此时表示随机变量之间完全相关。此时的联合分布概率函数形状为：曲面在 $z=0$ 平面的截面是直线 $y=x$，相当于圆形沿着直线 $y=x$ 方向压缩成一条直线 。
由于 $\rho=1$ 会导致除数为 0，因此这里给出 $\rho=0.9$：
<img src="https://img-blog.csdnimg.cn/4e1d470b139c45e8abfd5b320c90139a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZW_6Lev5ryr5ryrMjAyMQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width=36%>

3. 多维正态随机变量 $(X_1,X_2,\cdots,X_n)$，引入列矩阵：
$$\mathbf{\vec x}=\begin{bmatrix}
x_1 \\
x_2 \\
\vdots\\
x_n
\end{bmatrix} \quad
\mathbf{\vec \mu}=\begin{bmatrix}
\mu_1 \\ 
\mu_2\\
\vdots\\
\mu_n
\end{bmatrix}=\begin{bmatrix}
\mathbb E[X_1]  \\ 
\mathbb E[X_2] \\
\vdots\\
\mathbb E[X_n] 
\end{bmatrix}\tag{4-7}$$
 $\pmb{\Sigma}$ 为 $(X_1,X_2,\cdots,X_n)$ 的协方差矩阵。则：
$$p(x_1,x_2,x_3,\cdots,x_n)=\frac {1}{(2\pi)^{n/2}(\det \mathbf \Sigma)^{1/2}} \exp \{- \frac 12( \mathbf {\vec x}- \mathbf {\vec \mu})^{T}\mathbf \Sigma^{-1}( \mathbf {\vec x}- \mathbf {\vec \mu})\}\tag{4-8}$$
记做 ：$\mathcal N(\mathbf{\vec x};\mathbf{\vec \mu},\mathbf\Sigma) =\sqrt{\frac{1}{(2\pi)^{n}det(\mathbf\Sigma)}}\exp\left(-\frac 12(\mathbf{\vec x-\vec \mu})^{T}\mathbf\Sigma^{-1}(\mathbf{\vec x-\vec \mu})\right)$。

 4. $n$ 维正态变量具有下列四条性质：
- $n$ 维正态变量的每一个分量都是正态变量；反之，若 $X_1,X_2,\cdots,X_n$ 都是正态变量，且相互独立，则 $(X_1,X_2,\cdots,X_n)$ 是 $n$ 维正态变量。
 - $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布的充要条件是：$X_1,X_2,\cdots,X_n$  的任意线性组合：$l_1X_1+l_2X_2+\cdots+l_nX_n$ 服从一维正态分布，其中 $l_1,l_2,\cdots,l_n$ 不全为 0 。
- 若 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布，设 $Y_1,Y_2,\cdots,Y_k$ 是 $X_j,j=1,2,\cdots,n$ 的线性函数，则 $(Y_1,Y_2,\cdots,Y_k)$ 也服从多维正态分布。
这一性质称为正态变量的线性变换不变性。
- 设 $(X_1,X_2,\cdots,X_n)$ 服从 $X_1,X_2,\cdots,X_n$ 维正态分布，则 $(X_1,X_2,\cdots,X_n)$ 相互独立  $\Longleftrightarrow X_1,X_2,\cdots,X_n$ 两两不相关。

> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更多常见概率分布，请阅读：[统计学中常用的分布族](https://blog.csdn.net/xq151750111/article/details/120331890?spm=1001.2014.3001.5502)

____

## 5 先验分布与后验分布
1. 在贝叶斯学派中，<font color=#9900CC><strong>先验分布+数据（似然）= 后验分布 </font></strong>。
2. 例如：假设需要识别一大箱苹果中的好苹果、坏苹果的概率。
- 根据你对苹果好、坏的认知，给出先验分布为：50个好苹果和50个坏苹果。
- 现在你拿出10个苹果，发现有：8个好苹果，2个坏苹果。根据数据，你得到后验分布为：58个好苹果，52个坏苹果
- 再拿出10个苹果，发现有：9个好苹果，1个坏苹果。根据数据，你得到后验分布为：67个好苹果，53个坏苹果
- 这样不断重复下去，不断更新后验分布。当一箱苹果清点完毕，则得到了最终的后验分布。
在这里：
- 如果不使用先验分布，仅仅清点这箱苹果中的好坏，则得到的分布只能代表这一箱苹果。
- 采用了先验分布之后得到的分布，可以认为是所有箱子里的苹果的分布。
- 当采用先验分布时：给出的好、坏苹果的个数（也就是频数）越大，则先验分布越占主导地位。
3. 假设好苹果的概率为 $p$，则抽取 $N$ 个苹果中，好苹果个数为 $k$ 个的概率为一个二项分布：
$$Binom(k\mid p;N)=C_N^kp^k(1-p)^{N-k}$$
其中 $C_N^k$ 为组合数。
4. 现在的问题是：好苹果的概率 $p$ 不再固定，而是服从一个分布。
假设好苹果的概率 $p$ 的先验分布为贝塔分布：$Beta(p; \alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}$。
则后验概率为：
$$P(p\mid k; N,\alpha,\beta)=\frac{P(k\mid p; N)\times P(p; \alpha,\beta)}{P(k; N,\alpha,\beta)} \\
\propto P(k\mid p; N)\times P(p; \alpha,\beta)=C_N^kp^k(1-p)^{N-k}\times \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}\\
\propto p^{k+\alpha-1}(1-p)^{N-k+\beta-1}\tag{5-1}$$
归一化之后，得到后验概率为：
$$P(p\mid k;N,\alpha,\beta)=\frac{\Gamma(\alpha+\beta+N)}{\Gamma(\alpha+k)\Gamma(\beta+N-k)}p^{k+\alpha-1}(1-p)^{N-k+\beta-1}​\tag{5-2}$$
5. 好苹果概率 $p$ 的先验分布的期望为：$\mathbb E[p]=\frac{\alpha}{\alpha+\beta}$。好苹果概率 $p$ 的后验分布的期望为：$\mathbb E[p\mid k]=\frac{\alpha+k}{\alpha+\beta+N}$。

- 根据上述例子所述：
	- 好苹果的先验概率的期望为 $\frac {50}{50+50}=\frac 12$
	- 进行第一轮数据校验之后，好苹果的后验概率的期望为 $\frac {50+8}{50+50+10}=\frac{58}{110}$
- 如果将 $\alpha$ 视为先验的好苹果数量，$\beta$ 视为先验的坏苹果数量，$N$ 表示箱子中苹果的数量，$k$ 表示箱子中的好苹果数量（相应的，$N-k$ 就是箱子中坏苹果的数量）。则：好苹果的先验概率分布的期望、后验概率分布的期望符合人们的生活经验。
- 这里使用先验分布和后验分布的期望，因为 $p$ 是一个随机变量。若想通过一个数值来刻画好苹果的可能性，则用期望较好。
6. 更一般的，如果苹果不仅仅分为好、坏两种，而是分作尺寸1、尺寸2、……尺寸K等。则 $N$ 个苹果中，有 $m_1$ 个尺寸1的苹果、$m_2$ 个尺寸2的苹果……$m_K$ 个尺寸 $K$  的苹果的概率服从多项式分布：
$$Mult(m_1,m_2,\cdots,m_K;\vec\mu,N)=\frac{N!}{m_1!m_2!\cdots m_K!}\prod_{k=1}^{K}\mu_k^{m_k}\tag{5-3}$$
其中苹果为尺寸1的概率为 $\mu_1$， 尺寸2的概率为 $\mu_2$，……尺寸 $K$ 的概率为 $\mu_K$，$N=\sum_{k=1}^Km_k$ 
- 假设苹果尺寸的先验概率分布为狄利克雷分布：$Dir(\vec\mu;\vec\alpha)=\frac{\Gamma(\sum_{k=1}^{K}\alpha_k)}{\sum_{k=1}^{K}\Gamma(\alpha_k)}\prod_{k=1}^{K}\mu_k^{\alpha_k-1}$。
苹果尺寸的先验概率分布的期望为：$\mathbb E[\vec\mu]=\left(\frac{\alpha_1}{\sum_{k=1}^K\alpha_k},\frac{\alpha_2}{\sum_{k=1}^K\alpha_k},\cdots,\frac{\alpha_K}{\sum_{k=1}^K\alpha_k}\right)$。

- 则苹果尺寸的后验概率分布也为狄里克雷分布：$Dir(\vec\mu;\vec\alpha+\mathbf{\vec m})=\frac{\Gamma(N+\sum_{k=1}^{K}\alpha_k)}{\sum_{k=1}^{K}\Gamma(\alpha_k+m_k)}\prod_{k=1}^{K}\mu_k^{\alpha_k+m_k-1}$。
苹果尺寸的后验概率分布的期望为：$\mathbb E[\vec\mu]=\left(\frac{\alpha_1+m_1}{N+\sum_{k=1}^K\alpha_k},\frac{\alpha_2+m_2}{N+\sum_{k=1}^K\alpha_k},\cdots,\frac{\alpha_K+m_K}{N+\sum_{k=1}^K\alpha_k}\right)$

___

## 6 随机过程
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随机过程（Stochastic Process）是一组随机变量 $X_t$ 的集合，其中 $t$ 属于一个索引（index）集合 $\mathcal{T}$。索引集合 $\mathcal{T}$ 可以定义在时间域或者空间域，但一般为时间域，以实数或正数表示。当 $t$ 为实数时，随机过程为连续随机过程；当 $t$ 为整数时，为离散随机过程。日常生活中的很多例子包括股票的波动、语音信号、身高的变化等都可以看作是随机过程。常见的和时间相关的随机过程模型包括伯努利过程、随机游走（Random Walk）、马尔可夫过程等。和空间相关的随机过程通常称为随机场（Random Field）。比如一张二维的图片，每个像素点（变量）通过空间的位置进行索引，这些像素就组成了一个随机过程。

### 6.1 马尔科夫过程
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;马尔可夫性质在随机过程中，马尔可夫性质（Markov Property）是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。以离散随机过程为例，假设随机变量 $X_0,X_1,\cdots,X_T$ 构成一个随机过程。这些随机变量的所有可能取值的集合被称为状态空间（State Space）。如果 $X_{t+1}$ 对于过去状态的条件概率分布仅是 $X_t$ 的一个函数，则
$$P(X_{t+1}=x_{t+1}|X_{0:t}=x_{0:t})=P(X_{t+1}=x_{t+1}|X_{t}=x_{t})\tag{6-1}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中 $X_{0:t}$ 表示变量集合 $X_0,X_1,\cdots,X_T$；$x_{0:t}$ 为在状态空间中的状态序列。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;马尔可夫性质也可以描述为<font color=#9900CC><strong>给定当前状态时，将来的状态与过去状态是条件独立的。</font></strong>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;离散时间的马尔可夫过程也称为马尔可夫链（Markov Chain）。如果一个马尔可夫链的条件概率
$$P(X_{t+1}=s|X_{t}=s')=m_{ss'}\tag{6-2}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只和状态 $s$ 和 $s'$ 相关， 和时间 $t$ 无关， 则称为时间同质的马尔可夫链（Time Homogeneous Markov Chain），其中 $m_{ss'}$ 称为状态转移概率。如果状态空间大小 $𝐾$ 是有限的，状态转移概率可以用一个矩阵 $𝑴 \in \mathbb{R}^{K\times K}$ 表示，称为状态转移矩阵（Transition Matrix），其中元素$m_{ij}$ 表示状态 $s_i$ 转移到状态 $s_j$ 的概率。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设状态空间大小为 $𝐾$，向量 $\pmb{\pi} = [\pi_1,\cdots,\pi_K]^T$ 为状态空间中的一个分布，满足 $0 ≤ \pi_k ≤ 1$ 和 $\sum_{k=1}^{K}\pi_k=1$。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于状态转移矩阵为 $\pmb{M}$ 的时间同质的马尔可夫链，如果存在一个分布 $\pi$ 满足
$$\pmb{\pi}=\pmb{M\pi}\tag{6-3}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即分布 $\pmb{\pi}$ 就称为该马尔可夫链的平稳分布（Stationary Distribution）。根据特征向量的定义可知，$\pmb{\pi}$ 为矩阵 $\pmb{M}$ 的（归一化）的对应特征值为1 的特征向量。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果一个马尔可夫链的状态转移矩阵 $\pmb{M}$ 满足所有状态可遍历性以及非周期性，那么对于任意一个初始状态分布$\pmb{\pi}^0$，在经过一定时间的状态转移之后，都会收敛到平稳分布，即
$$\pmb{\pi}= \lim_{N\to\infty}\pmb{M}^N\pmb{\pi}^{(0)}\tag{6-4}$$

### 6.2 高斯过程
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高斯过程（Gaussian Process）也是一种应用广泛的随机过程模型。假设有一组连续随机变量 $X_0,X_1,\cdots,X_T$，如果由这组随机变量构成的任一有限集合
$$X_{t_1,\cdots,t_N}=[X_{t_1},\cdots,X_{t_N}]^T,1\leq N \leq T\tag{6-5}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;都服从一个多元正态分布，那么这组随机变量为一个随机过程。高斯过程也可以定义为：如果 $X_{t_1},\cdots,X_{t_N}$ 的任一线性组合都服从一元正态分布，那么这组随机变量为一个随机过程。
 
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高斯过程回归（Gaussian Process Regression）是利用高斯过程来对一个函数分布进行建模。和机器学习中参数化建模（比如贝叶斯线性回归）相比，高斯过程是一种非参数模型，可以拟合一个黑盒函数，并给出拟合结果的置信度。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设一个未知函数 $f(\pmb{x})$ 服从高斯过程，且为平滑函数。如果两个样本 $\pmb{x}_1,\pmb{x}_2$ 比较接近，那么对应的$f(\pmb{x}, \pmb{x}_2)$ 也比较接近。假设从函数 $𝑓(\pmb{x})$ 中采样有限个样本 $\pmb{X}= [\pmb{x}_1, \pmb{x}_2, \cdots,\pmb{x}_N]$，这 $N$ 个点服从一个多元正态分布，
$$[f(\pmb{x}_1), f(\pmb{x}_2), \cdots,f(\pmb{x}_N)]^T\sim N(\mu(X),K(X,X))\tag{6-6}$$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中 $\pmb{\mu}(\pmb{X}) = [\mu_{(\boldsymbol{x}_1)}, \mu_{(\boldsymbol{x}_2)},\cdots,\mu_{(\boldsymbol{x}_N)}]^T$ 是均值向量，$\pmb{K}(\pmb{X}, \pmb{X}) = [k(\pmb{x}_i, \pmb{x}_j)]_{N\times N}$ 是协方差矩阵，$k(\pmb{x}_i, \pmb{x}_j)$ 为核函数，可以衡量两个样本的相似度。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在高斯过程回归中，一个常用的核函数是平方指数（Squared Exponential）函数
$$k(\pmb{x}_i, \pmb{x}_j)=exp(\dfrac{-||\pmb{x}_i-\pmb{x}_j||^2}{2l^2})\tag{6-7}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中 $l$ 为超参数。当 $\pmb{x}_i$ 和 $\pmb{x}_j$ 越接近，其核函数的值越大，表明 $f(\pmb{x}_i)$ 和 $f(\pmb{x}_j)$ 越相关。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设 $f(\pmb{x})$ 的一组带噪声的观测值为 $\{(\pmb{x}_n,y_n)\}_{n=1}^{N}$，其中 $y_n\sim N(f(\pmb{x}_n,\sigma^2)$ 为 $f(\pmb{x}_n)$ 的观测值，服从正态分布，$\sigma^2$ 为噪声方差。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个新的样本点 $\pmb{x}^*$，我们希望预测 $f(\pmb{x}^*)$ 的观测值 $y^*$。令向量 $𝒚 =[y_1,y_2,\cdots,y_N]^T$ 为已有的观测值，根据高斯过程的假设，$[\pmb{y}; y^*]$ 满足
$$ \begin{bmatrix} \pmb{y} \\y^*\end{bmatrix}\sim \left(\begin{bmatrix} \pmb{\mu}(\pmb{X}) \\\mu(\pmb{x}^*)\end{bmatrix},\begin{bmatrix} \pmb{K}(\pmb{X},\pmb{X})+\sigma^2\pmb{I} & \pmb{K}(\pmb{x}^*, \pmb{X})^T \\\pmb{K}(\pmb{x}^*, \pmb{X}) & k(\pmb{x}^*,\pmb{x}^*)\end{bmatrix} \right) \tag{6-8}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中 $\pmb{K}(\pmb{x}^*, \pmb{X})=[k(\pmb{x}^*,\pmb{x}_1),\cdots,k(\pmb{x}^*,\pmb{x}_n)]$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据上面的联合分布，$y^*$ 的后验分布为
$$p(y^*|\pmb{X},\pmb{y})=N(\hat{\mu},\hat{\sigma}^2)\tag{6-9}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中均值 $\hat{\mu}$ 和方差 $\hat{\sigma}$ 为
$$\hat{\mu}=\pmb{K}(\pmb{x}^*, \pmb{X})(\pmb{K}(\pmb{X}, \pmb{X})+\sigma^2\pmb{I})^{-1}(\pmb{y}-\pmb{\mu}(\pmb{X}))+\mu(\pmb{x}^*)\\
\hat{\sigma}^2=k(\pmb{x}^*,\pmb{x}^*)-\pmb{K}(\pmb{x}^*, \pmb{X})(\pmb{K}(\pmb{X}, \pmb{X})+\sigma^2\pmb{I})^{-1}\pmb{K}(\pmb{x}^*, \pmb{X})^T\tag{6-10}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从公式可以看出，均值函数 $\pmb{\mu}(\pmb{x})$ 可以近似地互相抵消。在实际应用中，一般假设 $\mu(\pmb{x})=0$，均值 $\hat{\mu}$ 可以将简化为
$$\hat{\mu}=\pmb{K}(\pmb{x}^*, \pmb{X})(\pmb{K}(\pmb{X}, \pmb{X})+\sigma^2\pmb{I})^{-1}\pmb{y}\tag{6-11}$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高斯过程回归可以认为是一种有效的贝叶斯优化方法，广泛地应用于机器学习中。

____

## 7 信息论
1. 信息论背后的原理是：从不太可能发生的事件中能学到更多的有用信息。
- 发生可能性较大的事件包含较少的信息。
- 发生可能性较小的事件包含较多的信息。
- 独立事件包含额外的信息 。
2. 对于事件 $X=x$，定义自信息`self-information`为：$I(x)=-\log P(x)$。
自信息（Self Information）表示一个随机事件所包含的信息量。一个随机事件发生的概率越高，其自信息越低。如果一个事件必然发生，其自信息为0。在自信息的定义中，对数的底可以使用2、自然常数 $e$ 或10。当底为2时，自信息的单位为`bit`；当底为 $e$ 时，自信息的单位为`nat`。
自信息仅仅处理单个输出，但是如果计算自信息的期望，它就是熵：
$$H(X)=\mathbb E_{X\sim P(X)}[I(x)]=-\mathbb E_{X\sim P(X)}[\log P(x)]\tag{7-1}$$
记作 $H(P)$。
- 熵越高，则随机变量的信息越多；熵越低，则随机变量的信息越少。如果变量 $X$ 当且仅当在 $x$ 时 $P(x) = 1$，则熵为0。也就是说，对于一个确定的信息，其熵为0，信息量也为0。如果其概率分布为一个均匀分布，则熵最大。
- 熵刻画了按照真实分布 $P$ 来识别一个样本所需要的编码长度的期望（即平均编码长度）。
如：含有4个字母`(A,B,C,D)`的样本集中，真实分布 $P=(\frac{1}{2}, \frac{1}{2}, 0, 0)$，则只需要1位编码即可识别样本。
- 对于离散型随机变量 $X$，假设其取值集合大小为 $K$，则可以证明：$0\le H(X)\le \log K$  

3. 对于随机变量 $X$ 和 $Y$，条件熵 $H(Y|X)$ 表示：已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。
它定义为：$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的期望：
$$H(Y\mid X) = \mathbb E_{X\sim P(X)}[ H(Y\mid X=x)]=-\mathbb E_{(X,Y)\sim P(X,Y)} \log P(Y\mid X)\tag{7-2}$$
- 对于离散型随机变量，有：
$$H(Y\mid X) = \sum_xp(x) H(Y\mid X=x)=-\sum_x\sum_y p(x,y)\log p(y\mid x)\tag{7-3}$$
- 对于连续型随机变量，有：
$$H(Y\mid X) = \int p(x) H(Y\mid X=x) dx=-\int\int p(x,y)\log p(y\mid x) dx dy\tag{7-4}$$
4. 根据定义可以证明：$H(X,Y)=H(Y\mid X)+H(X)$。
即：描述 $X$ 和 $Y$ 所需要的信息是：描述 $X$ 所需要的信息加上给定 $X$ 条件下描述 $Y$ 所需的额外信息。
5. `KL`散度（也称作相对熵）：对于给定的随机变量 $X$，它的两个概率分布函数 $P(X)$ 和 $Q(X)$ 的区别可以用`KL`散度来度量：
$$D_{KL}(P||Q)=\mathbb E_{X\sim P(X)}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb E_{X\sim P(X)}\left[\log  P(x) -\log  Q(x) \right]$$
- `KL`散度非负：当它为 0 时，当且仅当`P`和`Q`是同一个分布（对于离散型随机变量），或者两个分布几乎处处相等（对于连续型随机变量）。
- `KL`散度不对称：$D_{KL}(P||Q) \neq D_{KL}(Q||P)$。
直观上看对于 $D_{KL}(P||Q)$，当 $P(x)$ 较大的地方，$Q(x)$也应该较大，这样才能使得 $P(x)\log\frac {P(x)}{Q(x)}$ 较小。
对于 $P(x)$ 较小的地方，$Q(x)$ 就没有什么限制就能够使得 $P(x)\log\frac {P(x)}{Q(x)}$ 较小。这就是`KL`散度不满足对称性的原因。
6. 交叉熵`cross-entropy`：$H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb E_{X\sim P(X)}\log Q(x)$。
- 交叉熵刻画了使用错误分布 Q 来表示真实分布 P 中的样本的平均编码长度。
 - $D_{KL(P||Q)}$ 刻画了错误分布 Q 编码真实分布 P 带来的平均编码长度的增量。
示例：假设真实分布 P 为混合高斯分布，它由两个高斯分布的分量组成。如果希望用普通的高斯分布 Q 来近似 P，则有两种方案
$$Q_1^* = \arg\min _Q D_{KL}(P||Q)\\
Q_2^* = \arg\min _Q D_{KL}(Q||P)\tag{7-5}$$
<img src="https://img-blog.csdnimg.cn/829efbd74cf24c978b76c27fcf148df0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZW_6Lev5ryr5ryrMjAyMQ==,size_17,color_FFFFFF,t_70,g_se,x_16#pic_center" width=30%>
- 如果选择 $Q_1^*$，则：
	- 当 $P(x)$ 较小的时候，$Q(x)$ 必须较小。如果 $P(x)$ 较小的时 $Q(x)$ 较大，则 $P(x)\log\frac {P(x)}{Q(x)}$ 较大。
	- 当 $P(x)$ 较大的时候，$Q(x)$ 可以较大，也可以较小。
因此 $Q_1^*$ 会贴近 $P(x)$ 的峰值。由于 $Q_1^*$ 的峰值有两个，因此 $Q_1^*$ 无法偏向任意一个峰值，最终结果就是 $Q_1^*$ 的峰值在 $P(x)$ 的两个峰值之间。
<img src="https://img-blog.csdnimg.cn/814bffa56cbd411887066143b51ad0f7.png#pic_center" width=30%>

- 如果选择 $Q_2^*$，则：
	- 当 $P(x)$ 较大的时候 $Q(x)$ 也必须较大 。如果 $P(x)$ 较大时 $Q(x)$ 较小，则 $P(x)\log\frac {Q(x)}{P(x)}$ 较大。
	- 当 $P(x)$ 较小的时候 $Q(x)$ 可以较大，也可以较小。
因此 $Q_2^*$ 会贴近 $P(x)$ 的谷值。最终结果就是 $Q_2^*$ 会贴合 $P(x)$ 峰值的任何一个。
<img src="https://img-blog.csdnimg.cn/8c32c975e9a04afbb79745b6b0ff0d6a.png#pic_center" width=30%>
- 绝大多数场合使用 $D_{KL}(P||Q)$，原因是：当用分布 Q 拟合 P 时我们希望对于常见的事件，二者概率相差不大。

___


## 8 其它
1. 假设随机变量 $X,Y$ 满足 $Y=g(X)$，且函数 $g(\cdot)$ 满足：处处连续、可导、且存在反函数。 则有：
$$p_X(x)=p_Y(g(x)) \left|\frac{\partial g(x)}{\partial x}\right|\tag{8-1}$$
或者等价地（其中 $g^{-1}(\cdot)$ 为反函数）：
$$p_Y(y)=p_X(g^{-1}(y)) \left|\frac{\partial x}{\partial y}\right|\tag{8-2}$$
- 如果扩展到高维空间，则有：
$$p_X(\mathbf{\vec x})=p_Y(g(\mathbf{\vec x})) \left|\det\left(\frac{\partial g(\mathbf{\vec x})}{\partial \mathbf{\vec x}}\right)\right|\tag{8-3}$$
- 并不是 $p_Y(y)=p_X(g^{-1}(y))$，这是因为 $g(\cdot)$ 引起了空间扭曲，从而导致 $\int p_X(g(x))dx \neq 1$。
根据 $|p_Y(g(x))dy|=|p_X(x)dx|$，求解该方程，即得到上述解。

2. 机器学习中不确定性有三个来源：
- 模型本身固有的随机性。如：量子力学中的粒子动力学方程。
- 不完全的观测。即使是确定性系统，当无法观测所有驱动变量时，结果也是随机的。
- 不完全建模。有时必须放弃一些观测信息。
如机器人建模中：虽然可以精确观察机器人周围每个对象的位置，但在预测这些对象将来的位置时，对空间进行了离散化。则位置预测将带有不确定性。